{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is used to train unimodal LSTM autoencoders and save the encoded representation. It also computes the FV from the encoded representation, performs feature selection and Random Forest Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n",
      "/home/ceccarelli/.local/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from os import listdir\n",
    "import os \n",
    "from os.path import isfile, join\n",
    "from scipy.signal import savgol_filter\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import os\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import itertools as it\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Masking\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.utils import plot_model\n",
    "from keras import optimizers\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.mixture import GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "# read and threshold output.\n",
    "# if less or eq to 7 put 0 (not depressed)\n",
    "# else put 1 \n",
    "DATA_PATH = \"TemporalData60\"\n",
    "DATA_PATH2 = \"FINAL_RESULTS_60\"\n",
    "onlyfiles = [f for f in listdir(DATA_PATH) if isfile(join(DATA_PATH, f))]\n",
    "x_train = list()\n",
    "x_train_length = list()\n",
    "y_train = list()\n",
    "\n",
    "for i in range(len(onlyfiles)):\n",
    "    tmp =  np.load(os.path.join(DATA_PATH, onlyfiles[i]))\n",
    "    x_train.append(tmp)\n",
    "    x_train_length.append(tmp.shape[0])\n",
    "    label = tmp[0,0,-1]\n",
    "    if (label <= 7):\n",
    "        y_train.append(0)\n",
    "    else:\n",
    "        y_train.append(1)\n",
    "        \n",
    "        \n",
    "print (len(x_train))\n",
    "print (len(x_train_length))\n",
    "print (len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data for autoencoder is  (710102, 60, 66)\n"
     ]
    }
   ],
   "source": [
    "X = np.vstack((x_train))\n",
    "print (\"Shape of data for autoencoder is \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of unimodal data for autoencoder is  (710102, 60, 13)\n"
     ]
    }
   ],
   "source": [
    "#subset the relevant features\n",
    "feature_type = \"mfcc\"\n",
    "if (feature_type == \"fidget\"):\n",
    "    X = X[:,:,:-1]\n",
    "    X = X[:,:,:9]\n",
    "elif (feature_type == \"gaze\"):\n",
    "\n",
    "    X = X[:,:,:-1]\n",
    "    X = X[:,:,9:17]\n",
    "elif (feature_type == \"au\"):\n",
    "\n",
    "    X = X[:,:,:-1]\n",
    "    X = X[:,:,17:52]\n",
    "else:\n",
    "   \n",
    "    X = X[:,:,:-1]\n",
    "    X = X[:,:,52:]\n",
    "        \n",
    "    \n",
    "print (\"Shape of unimodal data for autoencoder is \", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden dimension 5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "----------------------------------------\n",
      "autoencoder\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 60, 13)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 10)                760       \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 60, 10)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 60, 10)            640       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 60, 13)            143       \n",
      "=================================================================\n",
      "Total params: 1,543\n",
      "Trainable params: 1,543\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "----------------------------------------\n",
      "encoder\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 60, 13)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 10)                760       \n",
      "=================================================================\n",
      "Total params: 760\n",
      "Trainable params: 760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#MODEL DEFINITION\n",
    "hidden_ratio = 0.4\n",
    "timesteps, n_features = X.shape[1] , X.shape[2]\n",
    "dimensions_lstm = round(n_features*hidden_ratio)\n",
    "print ('Hidden dimension %i' %dimensions_lstm)\n",
    "# encoder part\n",
    "# input placeholder\n",
    "input_data = Input(shape=(timesteps, n_features))\n",
    "encoded = Bidirectional(LSTM(dimensions_lstm, activation='relu', return_sequences=False))(input_data)\n",
    "#connection between encoder and decoder\n",
    "middle_representation = RepeatVector(timesteps)(encoded)\n",
    "# decoder part\n",
    "decoded = Bidirectional(LSTM(dimensions_lstm, activation='relu', return_sequences=True))(middle_representation)\n",
    "decoded = TimeDistributed(Dense(n_features, activation = 'linear'))(decoded)\n",
    "autoencoder = Model(input_data, decoded)\n",
    "encoder = Model(input_data, encoded)\n",
    "autoencoder.compile(loss='mae', optimizer=\"adam\")\n",
    "print(\"--\" * 20)\n",
    "print(\"autoencoder\")\n",
    "print(autoencoder.summary())\n",
    "print(\"--\" * 20)\n",
    "print(\"encoder\")\n",
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4  Created \n"
     ]
    }
   ],
   "source": [
    "save_dir = 'FINAL_RESULTS_60'\n",
    "save_dir = save_dir + '/'+feature_type+'_timesteps'+str(timesteps)+'_HiddenRatio'+str(hidden_ratio)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    print(\"Directory \" , save_dir ,  \" Created \")\n",
    "else:    \n",
    "    print(\"Directory \" , save_dir ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "710102/710102 [==============================] - 45s 63us/step - loss: 23.0365\n",
      "\n",
      "Epoch 00001: loss improved from inf to 23.03653, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-01-23.04.hdf5\n",
      "Epoch 2/20\n",
      "710102/710102 [==============================] - 34s 47us/step - loss: 3.9685\n",
      "\n",
      "Epoch 00002: loss improved from 23.03653 to 3.96849, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-02-3.97.hdf5\n",
      "Epoch 3/20\n",
      "710102/710102 [==============================] - 45s 63us/step - loss: 2.8865\n",
      "\n",
      "Epoch 00003: loss improved from 3.96849 to 2.88653, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-03-2.89.hdf5\n",
      "Epoch 4/20\n",
      "710102/710102 [==============================] - 34s 48us/step - loss: 3.7576\n",
      "\n",
      "Epoch 00004: loss did not improve from 2.88653\n",
      "Epoch 5/20\n",
      "710102/710102 [==============================] - 49s 69us/step - loss: 3.2959\n",
      "\n",
      "Epoch 00005: loss did not improve from 2.88653\n",
      "Epoch 6/20\n",
      "710102/710102 [==============================] - 39s 56us/step - loss: 2.9809\n",
      "\n",
      "Epoch 00006: loss did not improve from 2.88653\n",
      "Epoch 7/20\n",
      "710102/710102 [==============================] - 36s 51us/step - loss: 2.7651\n",
      "\n",
      "Epoch 00007: loss improved from 2.88653 to 2.76507, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-07-2.77.hdf5\n",
      "Epoch 8/20\n",
      "710102/710102 [==============================] - 41s 58us/step - loss: 2.6422\n",
      "\n",
      "Epoch 00008: loss improved from 2.76507 to 2.64217, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-08-2.64.hdf5\n",
      "Epoch 9/20\n",
      "710102/710102 [==============================] - 31s 44us/step - loss: 2.5568\n",
      "\n",
      "Epoch 00009: loss improved from 2.64217 to 2.55676, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-09-2.56.hdf5\n",
      "Epoch 10/20\n",
      "710102/710102 [==============================] - 30s 42us/step - loss: 2.5079\n",
      "\n",
      "Epoch 00010: loss improved from 2.55676 to 2.50790, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-10-2.51.hdf5\n",
      "Epoch 11/20\n",
      "710102/710102 [==============================] - 33s 47us/step - loss: 2.4628\n",
      "\n",
      "Epoch 00011: loss improved from 2.50790 to 2.46276, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-11-2.46.hdf5\n",
      "Epoch 12/20\n",
      "710102/710102 [==============================] - 33s 47us/step - loss: 2.4306\n",
      "\n",
      "Epoch 00012: loss improved from 2.46276 to 2.43057, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-12-2.43.hdf5\n",
      "Epoch 13/20\n",
      "710102/710102 [==============================] - 33s 47us/step - loss: 2.3449\n",
      "\n",
      "Epoch 00013: loss improved from 2.43057 to 2.34485, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-13-2.34.hdf5\n",
      "Epoch 14/20\n",
      "710102/710102 [==============================] - 33s 47us/step - loss: 2.3096\n",
      "\n",
      "Epoch 00014: loss improved from 2.34485 to 2.30962, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-14-2.31.hdf5\n",
      "Epoch 15/20\n",
      "710102/710102 [==============================] - 30s 42us/step - loss: 2.3067\n",
      "\n",
      "Epoch 00015: loss improved from 2.30962 to 2.30670, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-15-2.31.hdf5\n",
      "Epoch 16/20\n",
      "710102/710102 [==============================] - 30s 42us/step - loss: 2.3052\n",
      "\n",
      "Epoch 00016: loss improved from 2.30670 to 2.30515, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-16-2.31.hdf5\n",
      "Epoch 17/20\n",
      "710102/710102 [==============================] - 30s 42us/step - loss: 2.3041\n",
      "\n",
      "Epoch 00017: loss improved from 2.30515 to 2.30413, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-17-2.30.hdf5\n",
      "Epoch 18/20\n",
      "710102/710102 [==============================] - 31s 44us/step - loss: 2.3034\n",
      "\n",
      "Epoch 00018: loss improved from 2.30413 to 2.30342, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-18-2.30.hdf5\n",
      "Epoch 19/20\n",
      "710102/710102 [==============================] - 31s 43us/step - loss: 2.3028\n",
      "\n",
      "Epoch 00019: loss improved from 2.30342 to 2.30280, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-19-2.30.hdf5\n",
      "Epoch 20/20\n",
      "710102/710102 [==============================] - 31s 43us/step - loss: 2.3023\n",
      "\n",
      "Epoch 00020: loss improved from 2.30280 to 2.30229, saving model to FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/weights-improvement-20-2.30.hdf5\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "csv_logger = CSVLogger(os.path.join(save_dir, \"logger.csv\"))\n",
    "checkpoint = ModelCheckpoint(os.path.join(save_dir, \"weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"), \n",
    "                             monitor='loss', \n",
    "                             verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [csv_logger, checkpoint]\n",
    "\n",
    "hist = autoencoder.fit(X, X, epochs=20,\n",
    "                batch_size=5120,\n",
    "                callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Loss'], loc='upper right')\n",
    "plt.savefig(os.path.join(save_dir, 'LOSS.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save_weights(os.path.join(save_dir, 'BI_LSTM_DYNAMICS.h5'))\n",
    "#autoencoder.load_weights(os.path.join(save_dir, 'BI_LSTM_DYNAMICS.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding representation\n",
      "Encoding done!\n"
     ]
    }
   ],
   "source": [
    "print ('Encoding representation')\n",
    "encoded_train = encoder.predict(X)\n",
    "print ('Encoding done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/encoding  Created \n"
     ]
    }
   ],
   "source": [
    "encoded_dir = 'FINAL_RESULTS_60'\n",
    "encoded_dir = encoded_dir + '/'+feature_type+'_timesteps'+str(timesteps)+'_HiddenRatio'+str(hidden_ratio)+'/encoding'\n",
    "if not os.path.exists(encoded_dir):\n",
    "    os.mkdir(encoded_dir)\n",
    "    print(\"Directory \" , encoded_dir ,  \" Created \")\n",
    "else:    \n",
    "    print(\"Directory \" , encoded_dir ,  \" already exists\")\n",
    "    \n",
    "np.save(os.path.join(encoded_dir, 'encoded_train'), encoded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_vector(xx, gmm, normalized=True):\n",
    "    \"\"\"Computes the Fisher vector on a set of descriptors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    xx: array_like, shape (N, D) or (D, )\n",
    "        The set of descriptors\n",
    "    gmm: instance of sklearn mixture.GMM object\n",
    "        Gauassian mixture model of the descriptors.\n",
    "    Returns\n",
    "    -------\n",
    "    fv: array_like, shape (K + 2 * D * K, )\n",
    "        Fisher vector (derivatives with respect to the mixing weights, means\n",
    "        and variances) of the given descriptors.\n",
    "    \"\"\"\n",
    "    xx = np.atleast_2d(xx)\n",
    "    N = xx.shape[0]\n",
    "\n",
    "    # Compute posterior probabilities.\n",
    "    Q = gmm.predict_proba(xx)  # NxK\n",
    "\n",
    "    # Compute the sufficient statistics of descriptors.\n",
    "    Q_sum = np.sum(Q, 0)[:, np.newaxis] / N\n",
    "    Q_xx = np.dot(Q.T, xx) / N\n",
    "    Q_xx_2 = np.dot(Q.T, xx ** 2) / N\n",
    "\n",
    "    # Compute derivatives with respect to mixing weights, means and variances.\n",
    "    d_pi = Q_sum.squeeze() - gmm.weights_\n",
    "    d_mu = Q_xx - Q_sum * gmm.means_\n",
    "    d_sigma = (\n",
    "        - Q_xx_2\n",
    "        - Q_sum * gmm.means_ ** 2\n",
    "        + Q_sum * gmm.covars_\n",
    "        + 2 * Q_xx * gmm.means_)\n",
    "\n",
    "    # Merge derivatives into a vector.\n",
    "    fisher =  np.hstack((d_pi, d_mu.flatten(), d_sigma.flatten()))\n",
    "    if normalized:\n",
    "            fisher = np.sqrt(np.abs(fisher)) * np.sign(fisher) # power normalization\n",
    "            fisher = fisher / np.linalg.norm(fisher, axis=0) # L2 norm\n",
    "            \n",
    "    return fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame2session(encoded_dir, x_train_length):\n",
    "    \n",
    "    tmp = list()\n",
    "    X_train_frame = np.load(encoded_dir + \"/encoded_train.npy\")\n",
    "    indices = np.zeros((len(x_train_length)+1))\n",
    "    indices[0] = 0\n",
    "    for i in range(1, len(x_train_length)):\n",
    "        indices[i] = indices[i-1] + x_train_length[i-1]\n",
    "    indices[len(x_train_length)] = sum(x_train_length)-1 \n",
    "    for i in range(len(indices)-1):\n",
    "        tmp.append(X_train_frame[int(indices[i]):int(indices[i+1]),:])\n",
    "        \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "X_frame_train = frame2session(encoded_dir, x_train_length)\n",
    "print (len(X_frame_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  FINAL_RESULTS_60/mfcc_timesteps60_HiddenRatio0.4/fisherVectors  Created \n",
      "Computing kernel 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ceccarelli/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class GMM is deprecated; The class GMM is deprecated in 0.18 and will be  removed in 0.20. Use class GaussianMixture instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/ceccarelli/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function distribute_covar_matrix_to_match_covariance_type is deprecated; The function distribute_covar_matrix_to_match_covariance_typeis deprecated in 0.18 and will be removed in 0.20.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/ceccarelli/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing kernel 32\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "    \n",
    "save_dir = 'FINAL_RESULTS_60'\n",
    "save_dir = save_dir + '/'+feature_type+'_timesteps'+str(timesteps)+'_HiddenRatio'+str(hidden_ratio)+'/fisherVectors'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    print(\"Directory \" , save_dir ,  \" Created \")\n",
    "else:    \n",
    "    print(\"Directory \" , save_dir ,  \" already exists\")\n",
    "    \n",
    "    \n",
    "kernels = [16 , 32]\n",
    "for kernel in kernels:\n",
    "    \n",
    "    print ('Computing kernel %i' %kernel)\n",
    "    fv_train, fv_dev = [], []\n",
    "\n",
    "    for X_train in X_frame_train:\n",
    "        gmm = GMM(n_components=kernel, covariance_type='diag')\n",
    "        gmm.fit(X_train)\n",
    "        fv = fisher_vector(X_train, gmm)\n",
    "        fv_train.append(fv)\n",
    "\n",
    "    np.save(os.path.join(save_dir, 'fisher_vector_train_%i' %kernel), fv_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 336)\n",
      "64\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -0.003196   0.016818   0.037713   0.044312  -0.020322   0.028315   \n",
      "1   0.053181   0.046687   0.042593  -0.033030  -0.074276   0.034757   \n",
      "2   0.033206   0.017512  -0.025823  -0.054084  -0.024345   0.042596   \n",
      "3  -0.025786   0.078076  -0.054355  -0.028245  -0.052873   0.034688   \n",
      "4  -0.029932   0.062247  -0.080130   0.043698   0.071922  -0.058681   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_326  feature_327  \\\n",
      "0  -0.069711   0.035199   0.033197   0.013107  ...     0.050233     0.019413   \n",
      "1   0.087899  -0.046566  -0.036739  -0.030427  ...     0.015559     0.008920   \n",
      "2   0.016318  -0.003803  -0.024744  -0.077192  ...     0.069301     0.027037   \n",
      "3  -0.016117   0.052303   0.021985  -0.068788  ...     0.023310    -0.007083   \n",
      "4   0.062502  -0.033216  -0.038009  -0.086980  ...     0.043495     0.027045   \n",
      "\n",
      "   feature_328  feature_329  feature_330  feature_331  feature_332  \\\n",
      "0     0.029348     0.023202     0.042520     0.019367     0.004782   \n",
      "1     0.024316     0.042281     0.033839    -0.056984     0.191080   \n",
      "2     0.034130     0.023389     0.049297     0.034148     0.050799   \n",
      "3     0.031607     0.022231     0.026331     0.032082     0.213731   \n",
      "4     0.031489     0.019121     0.049694     0.118842     0.128444   \n",
      "\n",
      "   feature_333  feature_334  feature_335  \n",
      "0     0.025864    -0.023476     0.022704  \n",
      "1    -0.054130     0.037036     0.025750  \n",
      "2     0.049187    -0.042954     0.025952  \n",
      "3    -0.018759     0.221059     0.032400  \n",
      "4     0.402873    -0.062313     0.034587  \n",
      "\n",
      "[5 rows x 336 columns]\n",
      "\n",
      "feature importance ranking\n",
      "1. feature 118 feature_118 (0.014207)\n",
      "2. feature 147 feature_147 (0.013800)\n",
      "3. feature 125 feature_125 (0.012676)\n",
      "4. feature 151 feature_151 (0.012441)\n",
      "5. feature 24 feature_24 (0.011832)\n",
      "6. feature 102 feature_102 (0.010172)\n",
      "7. feature 225 feature_225 (0.009952)\n",
      "8. feature 236 feature_236 (0.009305)\n",
      "9. feature 38 feature_38 (0.008079)\n",
      "10. feature 62 feature_62 (0.007966)\n",
      "11. feature 14 feature_14 (0.007819)\n",
      "12. feature 228 feature_228 (0.007437)\n",
      "13. feature 332 feature_332 (0.006985)\n",
      "14. feature 160 feature_160 (0.006966)\n",
      "15. feature 308 feature_308 (0.006879)\n",
      "16. feature 141 feature_141 (0.006834)\n",
      "17. feature 19 feature_19 (0.006816)\n",
      "18. feature 189 feature_189 (0.006578)\n",
      "19. feature 167 feature_167 (0.006504)\n",
      "20. feature 113 feature_113 (0.006488)\n",
      "21. feature 304 feature_304 (0.006042)\n",
      "22. feature 262 feature_262 (0.005836)\n",
      "23. feature 226 feature_226 (0.005725)\n",
      "24. feature 48 feature_48 (0.005571)\n",
      "25. feature 154 feature_154 (0.005564)\n",
      "26. feature 96 feature_96 (0.005558)\n",
      "27. feature 227 feature_227 (0.005554)\n",
      "28. feature 92 feature_92 (0.005532)\n",
      "29. feature 76 feature_76 (0.005494)\n",
      "30. feature 95 feature_95 (0.005452)\n",
      "31. feature 333 feature_333 (0.005423)\n",
      "32. feature 211 feature_211 (0.005368)\n",
      "33. feature 91 feature_91 (0.005357)\n",
      "34. feature 311 feature_311 (0.005256)\n",
      "35. feature 169 feature_169 (0.005134)\n",
      "36. feature 320 feature_320 (0.005007)\n",
      "37. feature 158 feature_158 (0.004990)\n",
      "38. feature 172 feature_172 (0.004972)\n",
      "39. feature 94 feature_94 (0.004925)\n",
      "40. feature 18 feature_18 (0.004925)\n",
      "41. feature 276 feature_276 (0.004842)\n",
      "42. feature 179 feature_179 (0.004834)\n",
      "43. feature 307 feature_307 (0.004823)\n",
      "44. feature 155 feature_155 (0.004806)\n",
      "45. feature 30 feature_30 (0.004728)\n",
      "46. feature 165 feature_165 (0.004641)\n",
      "47. feature 251 feature_251 (0.004627)\n",
      "48. feature 162 feature_162 (0.004618)\n",
      "49. feature 53 feature_53 (0.004489)\n",
      "50. feature 193 feature_193 (0.004463)\n",
      "51. feature 206 feature_206 (0.004445)\n",
      "52. feature 88 feature_88 (0.004430)\n",
      "53. feature 207 feature_207 (0.004355)\n",
      "54. feature 202 feature_202 (0.004343)\n",
      "55. feature 199 feature_199 (0.004334)\n",
      "56. feature 264 feature_264 (0.004274)\n",
      "57. feature 191 feature_191 (0.004213)\n",
      "58. feature 143 feature_143 (0.004211)\n",
      "59. feature 182 feature_182 (0.004187)\n",
      "60. feature 260 feature_260 (0.004157)\n",
      "61. feature 259 feature_259 (0.004146)\n",
      "62. feature 181 feature_181 (0.004074)\n",
      "63. feature 153 feature_153 (0.004057)\n",
      "64. feature 144 feature_144 (0.004012)\n",
      "65. feature 192 feature_192 (0.003984)\n",
      "66. feature 291 feature_291 (0.003950)\n",
      "67. feature 10 feature_10 (0.003928)\n",
      "68. feature 84 feature_84 (0.003866)\n",
      "69. feature 200 feature_200 (0.003862)\n",
      "70. feature 161 feature_161 (0.003848)\n",
      "71. feature 272 feature_272 (0.003840)\n",
      "72. feature 54 feature_54 (0.003817)\n",
      "73. feature 29 feature_29 (0.003816)\n",
      "74. feature 112 feature_112 (0.003776)\n",
      "75. feature 138 feature_138 (0.003759)\n",
      "76. feature 21 feature_21 (0.003756)\n",
      "77. feature 213 feature_213 (0.003754)\n",
      "78. feature 216 feature_216 (0.003695)\n",
      "79. feature 288 feature_288 (0.003673)\n",
      "80. feature 98 feature_98 (0.003656)\n",
      "81. feature 116 feature_116 (0.003653)\n",
      "82. feature 238 feature_238 (0.003620)\n",
      "83. feature 39 feature_39 (0.003589)\n",
      "84. feature 257 feature_257 (0.003588)\n",
      "85. feature 246 feature_246 (0.003561)\n",
      "86. feature 58 feature_58 (0.003498)\n",
      "87. feature 23 feature_23 (0.003458)\n",
      "88. feature 330 feature_330 (0.003430)\n",
      "89. feature 198 feature_198 (0.003424)\n",
      "90. feature 97 feature_97 (0.003418)\n",
      "91. feature 59 feature_59 (0.003406)\n",
      "92. feature 69 feature_69 (0.003396)\n",
      "93. feature 208 feature_208 (0.003395)\n",
      "94. feature 247 feature_247 (0.003368)\n",
      "95. feature 281 feature_281 (0.003357)\n",
      "96. feature 136 feature_136 (0.003342)\n",
      "97. feature 205 feature_205 (0.003305)\n",
      "98. feature 328 feature_328 (0.003297)\n",
      "99. feature 142 feature_142 (0.003292)\n",
      "100. feature 31 feature_31 (0.003287)\n",
      "(64, 50)\n",
      "(64, 336)\n",
      "64\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -0.003196   0.016818   0.037713   0.044312  -0.020322   0.028315   \n",
      "1   0.053181   0.046687   0.042593  -0.033030  -0.074276   0.034757   \n",
      "2   0.033206   0.017512  -0.025823  -0.054084  -0.024345   0.042596   \n",
      "3  -0.025786   0.078076  -0.054355  -0.028245  -0.052873   0.034688   \n",
      "4  -0.029932   0.062247  -0.080130   0.043698   0.071922  -0.058681   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_326  feature_327  \\\n",
      "0  -0.069711   0.035199   0.033197   0.013107  ...     0.050233     0.019413   \n",
      "1   0.087899  -0.046566  -0.036739  -0.030427  ...     0.015559     0.008920   \n",
      "2   0.016318  -0.003803  -0.024744  -0.077192  ...     0.069301     0.027037   \n",
      "3  -0.016117   0.052303   0.021985  -0.068788  ...     0.023310    -0.007083   \n",
      "4   0.062502  -0.033216  -0.038009  -0.086980  ...     0.043495     0.027045   \n",
      "\n",
      "   feature_328  feature_329  feature_330  feature_331  feature_332  \\\n",
      "0     0.029348     0.023202     0.042520     0.019367     0.004782   \n",
      "1     0.024316     0.042281     0.033839    -0.056984     0.191080   \n",
      "2     0.034130     0.023389     0.049297     0.034148     0.050799   \n",
      "3     0.031607     0.022231     0.026331     0.032082     0.213731   \n",
      "4     0.031489     0.019121     0.049694     0.118842     0.128444   \n",
      "\n",
      "   feature_333  feature_334  feature_335  \n",
      "0     0.025864    -0.023476     0.022704  \n",
      "1    -0.054130     0.037036     0.025750  \n",
      "2     0.049187    -0.042954     0.025952  \n",
      "3    -0.018759     0.221059     0.032400  \n",
      "4     0.402873    -0.062313     0.034587  \n",
      "\n",
      "[5 rows x 336 columns]\n",
      "\n",
      "feature importance ranking\n",
      "1. feature 147 feature_147 (0.020171)\n",
      "2. feature 24 feature_24 (0.013357)\n",
      "3. feature 125 feature_125 (0.012998)\n",
      "4. feature 118 feature_118 (0.012648)\n",
      "5. feature 102 feature_102 (0.011294)\n",
      "6. feature 236 feature_236 (0.010560)\n",
      "7. feature 113 feature_113 (0.009448)\n",
      "8. feature 189 feature_189 (0.009135)\n",
      "9. feature 62 feature_62 (0.008575)\n",
      "10. feature 38 feature_38 (0.008543)\n",
      "11. feature 151 feature_151 (0.008416)\n",
      "12. feature 225 feature_225 (0.007901)\n",
      "13. feature 332 feature_332 (0.007896)\n",
      "14. feature 95 feature_95 (0.007870)\n",
      "15. feature 311 feature_311 (0.007599)\n",
      "16. feature 211 feature_211 (0.006827)\n",
      "17. feature 226 feature_226 (0.006461)\n",
      "18. feature 228 feature_228 (0.006458)\n",
      "19. feature 154 feature_154 (0.006332)\n",
      "20. feature 18 feature_18 (0.006196)\n",
      "21. feature 172 feature_172 (0.006099)\n",
      "22. feature 308 feature_308 (0.006098)\n",
      "23. feature 91 feature_91 (0.006031)\n",
      "24. feature 144 feature_144 (0.006030)\n",
      "25. feature 14 feature_14 (0.005930)\n",
      "26. feature 262 feature_262 (0.005906)\n",
      "27. feature 48 feature_48 (0.005853)\n",
      "28. feature 165 feature_165 (0.005799)\n",
      "29. feature 88 feature_88 (0.005799)\n",
      "30. feature 141 feature_141 (0.005791)\n",
      "31. feature 307 feature_307 (0.005579)\n",
      "32. feature 92 feature_92 (0.005409)\n",
      "33. feature 19 feature_19 (0.005353)\n",
      "34. feature 76 feature_76 (0.005297)\n",
      "35. feature 288 feature_288 (0.005210)\n",
      "36. feature 229 feature_229 (0.005191)\n",
      "37. feature 167 feature_167 (0.005165)\n",
      "38. feature 304 feature_304 (0.005112)\n",
      "39. feature 160 feature_160 (0.005080)\n",
      "40. feature 158 feature_158 (0.005017)\n",
      "41. feature 179 feature_179 (0.004959)\n",
      "42. feature 333 feature_333 (0.004901)\n",
      "43. feature 198 feature_198 (0.004835)\n",
      "44. feature 169 feature_169 (0.004673)\n",
      "45. feature 111 feature_111 (0.004650)\n",
      "46. feature 330 feature_330 (0.004608)\n",
      "47. feature 84 feature_84 (0.004594)\n",
      "48. feature 66 feature_66 (0.004521)\n",
      "49. feature 192 feature_192 (0.004511)\n",
      "50. feature 94 feature_94 (0.004510)\n",
      "51. feature 233 feature_233 (0.004496)\n",
      "52. feature 240 feature_240 (0.004439)\n",
      "53. feature 193 feature_193 (0.004430)\n",
      "54. feature 257 feature_257 (0.004354)\n",
      "55. feature 276 feature_276 (0.004314)\n",
      "56. feature 271 feature_271 (0.004278)\n",
      "57. feature 259 feature_259 (0.004191)\n",
      "58. feature 53 feature_53 (0.004170)\n",
      "59. feature 116 feature_116 (0.004151)\n",
      "60. feature 320 feature_320 (0.004054)\n",
      "61. feature 251 feature_251 (0.003966)\n",
      "62. feature 209 feature_209 (0.003928)\n",
      "63. feature 150 feature_150 (0.003881)\n",
      "64. feature 30 feature_30 (0.003852)\n",
      "65. feature 325 feature_325 (0.003846)\n",
      "66. feature 39 feature_39 (0.003809)\n",
      "67. feature 96 feature_96 (0.003747)\n",
      "68. feature 323 feature_323 (0.003743)\n",
      "69. feature 273 feature_273 (0.003731)\n",
      "70. feature 216 feature_216 (0.003690)\n",
      "71. feature 235 feature_235 (0.003671)\n",
      "72. feature 170 feature_170 (0.003639)\n",
      "73. feature 194 feature_194 (0.003615)\n",
      "74. feature 263 feature_263 (0.003580)\n",
      "75. feature 191 feature_191 (0.003560)\n",
      "76. feature 16 feature_16 (0.003514)\n",
      "77. feature 146 feature_146 (0.003507)\n",
      "78. feature 203 feature_203 (0.003489)\n",
      "79. feature 246 feature_246 (0.003474)\n",
      "80. feature 153 feature_153 (0.003456)\n",
      "81. feature 161 feature_161 (0.003452)\n",
      "82. feature 122 feature_122 (0.003444)\n",
      "83. feature 43 feature_43 (0.003406)\n",
      "84. feature 291 feature_291 (0.003383)\n",
      "85. feature 173 feature_173 (0.003365)\n",
      "86. feature 44 feature_44 (0.003307)\n",
      "87. feature 310 feature_310 (0.003288)\n",
      "88. feature 328 feature_328 (0.003277)\n",
      "89. feature 143 feature_143 (0.003229)\n",
      "90. feature 31 feature_31 (0.003213)\n",
      "91. feature 59 feature_59 (0.003203)\n",
      "92. feature 199 feature_199 (0.003198)\n",
      "93. feature 206 feature_206 (0.003196)\n",
      "94. feature 234 feature_234 (0.003183)\n",
      "95. feature 98 feature_98 (0.003179)\n",
      "96. feature 182 feature_182 (0.003178)\n",
      "97. feature 200 feature_200 (0.003176)\n",
      "98. feature 33 feature_33 (0.003149)\n",
      "99. feature 207 feature_207 (0.003133)\n",
      "100. feature 237 feature_237 (0.003127)\n",
      "(64, 100)\n",
      "(64, 336)\n",
      "64\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -0.003196   0.016818   0.037713   0.044312  -0.020322   0.028315   \n",
      "1   0.053181   0.046687   0.042593  -0.033030  -0.074276   0.034757   \n",
      "2   0.033206   0.017512  -0.025823  -0.054084  -0.024345   0.042596   \n",
      "3  -0.025786   0.078076  -0.054355  -0.028245  -0.052873   0.034688   \n",
      "4  -0.029932   0.062247  -0.080130   0.043698   0.071922  -0.058681   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_326  feature_327  \\\n",
      "0  -0.069711   0.035199   0.033197   0.013107  ...     0.050233     0.019413   \n",
      "1   0.087899  -0.046566  -0.036739  -0.030427  ...     0.015559     0.008920   \n",
      "2   0.016318  -0.003803  -0.024744  -0.077192  ...     0.069301     0.027037   \n",
      "3  -0.016117   0.052303   0.021985  -0.068788  ...     0.023310    -0.007083   \n",
      "4   0.062502  -0.033216  -0.038009  -0.086980  ...     0.043495     0.027045   \n",
      "\n",
      "   feature_328  feature_329  feature_330  feature_331  feature_332  \\\n",
      "0     0.029348     0.023202     0.042520     0.019367     0.004782   \n",
      "1     0.024316     0.042281     0.033839    -0.056984     0.191080   \n",
      "2     0.034130     0.023389     0.049297     0.034148     0.050799   \n",
      "3     0.031607     0.022231     0.026331     0.032082     0.213731   \n",
      "4     0.031489     0.019121     0.049694     0.118842     0.128444   \n",
      "\n",
      "   feature_333  feature_334  feature_335  \n",
      "0     0.025864    -0.023476     0.022704  \n",
      "1    -0.054130     0.037036     0.025750  \n",
      "2     0.049187    -0.042954     0.025952  \n",
      "3    -0.018759     0.221059     0.032400  \n",
      "4     0.402873    -0.062313     0.034587  \n",
      "\n",
      "[5 rows x 336 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature importance ranking\n",
      "1. feature 151 feature_151 (0.014123)\n",
      "2. feature 118 feature_118 (0.014120)\n",
      "3. feature 125 feature_125 (0.013533)\n",
      "4. feature 102 feature_102 (0.012447)\n",
      "5. feature 62 feature_62 (0.011317)\n",
      "6. feature 147 feature_147 (0.009315)\n",
      "7. feature 24 feature_24 (0.009314)\n",
      "8. feature 225 feature_225 (0.008963)\n",
      "9. feature 113 feature_113 (0.008855)\n",
      "10. feature 158 feature_158 (0.008306)\n",
      "11. feature 167 feature_167 (0.008275)\n",
      "12. feature 14 feature_14 (0.008054)\n",
      "13. feature 311 feature_311 (0.007963)\n",
      "14. feature 236 feature_236 (0.007946)\n",
      "15. feature 38 feature_38 (0.007709)\n",
      "16. feature 262 feature_262 (0.007085)\n",
      "17. feature 48 feature_48 (0.006838)\n",
      "18. feature 95 feature_95 (0.006725)\n",
      "19. feature 226 feature_226 (0.006706)\n",
      "20. feature 333 feature_333 (0.006424)\n",
      "21. feature 19 feature_19 (0.006417)\n",
      "22. feature 92 feature_92 (0.006310)\n",
      "23. feature 76 feature_76 (0.006288)\n",
      "24. feature 144 feature_144 (0.006122)\n",
      "25. feature 332 feature_332 (0.006050)\n",
      "26. feature 228 feature_228 (0.006041)\n",
      "27. feature 308 feature_308 (0.005974)\n",
      "28. feature 31 feature_31 (0.005866)\n",
      "29. feature 141 feature_141 (0.005806)\n",
      "30. feature 304 feature_304 (0.005588)\n",
      "31. feature 189 feature_189 (0.005562)\n",
      "32. feature 276 feature_276 (0.005556)\n",
      "33. feature 18 feature_18 (0.005443)\n",
      "34. feature 259 feature_259 (0.005439)\n",
      "35. feature 257 feature_257 (0.005424)\n",
      "36. feature 154 feature_154 (0.005398)\n",
      "37. feature 194 feature_194 (0.005235)\n",
      "38. feature 165 feature_165 (0.005085)\n",
      "39. feature 133 feature_133 (0.004880)\n",
      "40. feature 209 feature_209 (0.004858)\n",
      "41. feature 211 feature_211 (0.004838)\n",
      "42. feature 260 feature_260 (0.004822)\n",
      "43. feature 307 feature_307 (0.004718)\n",
      "44. feature 193 feature_193 (0.004690)\n",
      "45. feature 88 feature_88 (0.004619)\n",
      "46. feature 192 feature_192 (0.004594)\n",
      "47. feature 146 feature_146 (0.004589)\n",
      "48. feature 43 feature_43 (0.004263)\n",
      "49. feature 91 feature_91 (0.004218)\n",
      "50. feature 182 feature_182 (0.004196)\n",
      "51. feature 172 feature_172 (0.004173)\n",
      "52. feature 54 feature_54 (0.004150)\n",
      "53. feature 111 feature_111 (0.004133)\n",
      "54. feature 160 feature_160 (0.004113)\n",
      "55. feature 69 feature_69 (0.003983)\n",
      "56. feature 263 feature_263 (0.003955)\n",
      "57. feature 66 feature_66 (0.003934)\n",
      "58. feature 127 feature_127 (0.003887)\n",
      "59. feature 142 feature_142 (0.003860)\n",
      "60. feature 4 feature_4 (0.003846)\n",
      "61. feature 138 feature_138 (0.003843)\n",
      "62. feature 185 feature_185 (0.003783)\n",
      "63. feature 94 feature_94 (0.003782)\n",
      "64. feature 212 feature_212 (0.003768)\n",
      "65. feature 222 feature_222 (0.003722)\n",
      "66. feature 10 feature_10 (0.003699)\n",
      "67. feature 229 feature_229 (0.003672)\n",
      "68. feature 39 feature_39 (0.003661)\n",
      "69. feature 53 feature_53 (0.003632)\n",
      "70. feature 251 feature_251 (0.003613)\n",
      "71. feature 255 feature_255 (0.003604)\n",
      "72. feature 96 feature_96 (0.003600)\n",
      "73. feature 52 feature_52 (0.003580)\n",
      "74. feature 272 feature_272 (0.003573)\n",
      "75. feature 98 feature_98 (0.003567)\n",
      "76. feature 199 feature_199 (0.003553)\n",
      "77. feature 207 feature_207 (0.003508)\n",
      "78. feature 179 feature_179 (0.003502)\n",
      "79. feature 288 feature_288 (0.003457)\n",
      "80. feature 107 feature_107 (0.003455)\n",
      "81. feature 328 feature_328 (0.003410)\n",
      "82. feature 44 feature_44 (0.003377)\n",
      "83. feature 99 feature_99 (0.003376)\n",
      "84. feature 116 feature_116 (0.003361)\n",
      "85. feature 119 feature_119 (0.003290)\n",
      "86. feature 37 feature_37 (0.003289)\n",
      "87. feature 217 feature_217 (0.003274)\n",
      "88. feature 330 feature_330 (0.003268)\n",
      "89. feature 287 feature_287 (0.003265)\n",
      "90. feature 280 feature_280 (0.003263)\n",
      "91. feature 216 feature_216 (0.003262)\n",
      "92. feature 299 feature_299 (0.003261)\n",
      "93. feature 271 feature_271 (0.003258)\n",
      "94. feature 30 feature_30 (0.003239)\n",
      "95. feature 246 feature_246 (0.003228)\n",
      "96. feature 293 feature_293 (0.003220)\n",
      "97. feature 16 feature_16 (0.003219)\n",
      "98. feature 176 feature_176 (0.003215)\n",
      "99. feature 220 feature_220 (0.003206)\n",
      "100. feature 254 feature_254 (0.003197)\n",
      "(64, 150)\n",
      "(64, 336)\n",
      "64\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -0.003196   0.016818   0.037713   0.044312  -0.020322   0.028315   \n",
      "1   0.053181   0.046687   0.042593  -0.033030  -0.074276   0.034757   \n",
      "2   0.033206   0.017512  -0.025823  -0.054084  -0.024345   0.042596   \n",
      "3  -0.025786   0.078076  -0.054355  -0.028245  -0.052873   0.034688   \n",
      "4  -0.029932   0.062247  -0.080130   0.043698   0.071922  -0.058681   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_326  feature_327  \\\n",
      "0  -0.069711   0.035199   0.033197   0.013107  ...     0.050233     0.019413   \n",
      "1   0.087899  -0.046566  -0.036739  -0.030427  ...     0.015559     0.008920   \n",
      "2   0.016318  -0.003803  -0.024744  -0.077192  ...     0.069301     0.027037   \n",
      "3  -0.016117   0.052303   0.021985  -0.068788  ...     0.023310    -0.007083   \n",
      "4   0.062502  -0.033216  -0.038009  -0.086980  ...     0.043495     0.027045   \n",
      "\n",
      "   feature_328  feature_329  feature_330  feature_331  feature_332  \\\n",
      "0     0.029348     0.023202     0.042520     0.019367     0.004782   \n",
      "1     0.024316     0.042281     0.033839    -0.056984     0.191080   \n",
      "2     0.034130     0.023389     0.049297     0.034148     0.050799   \n",
      "3     0.031607     0.022231     0.026331     0.032082     0.213731   \n",
      "4     0.031489     0.019121     0.049694     0.118842     0.128444   \n",
      "\n",
      "   feature_333  feature_334  feature_335  \n",
      "0     0.025864    -0.023476     0.022704  \n",
      "1    -0.054130     0.037036     0.025750  \n",
      "2     0.049187    -0.042954     0.025952  \n",
      "3    -0.018759     0.221059     0.032400  \n",
      "4     0.402873    -0.062313     0.034587  \n",
      "\n",
      "[5 rows x 336 columns]\n",
      "\n",
      "feature importance ranking\n",
      "1. feature 151 feature_151 (0.013639)\n",
      "2. feature 147 feature_147 (0.013006)\n",
      "3. feature 125 feature_125 (0.011185)\n",
      "4. feature 24 feature_24 (0.009869)\n",
      "5. feature 228 feature_228 (0.009528)\n",
      "6. feature 118 feature_118 (0.009520)\n",
      "7. feature 311 feature_311 (0.008891)\n",
      "8. feature 225 feature_225 (0.008823)\n",
      "9. feature 102 feature_102 (0.008361)\n",
      "10. feature 236 feature_236 (0.007877)\n",
      "11. feature 62 feature_62 (0.007838)\n",
      "12. feature 332 feature_332 (0.007706)\n",
      "13. feature 38 feature_38 (0.007643)\n",
      "14. feature 95 feature_95 (0.007468)\n",
      "15. feature 276 feature_276 (0.007270)\n",
      "16. feature 92 feature_92 (0.007125)\n",
      "17. feature 226 feature_226 (0.007022)\n",
      "18. feature 14 feature_14 (0.006901)\n",
      "19. feature 189 feature_189 (0.006887)\n",
      "20. feature 158 feature_158 (0.006877)\n",
      "21. feature 154 feature_154 (0.006756)\n",
      "22. feature 76 feature_76 (0.006628)\n",
      "23. feature 211 feature_211 (0.006600)\n",
      "24. feature 262 feature_262 (0.006349)\n",
      "25. feature 19 feature_19 (0.006289)\n",
      "26. feature 48 feature_48 (0.006097)\n",
      "27. feature 193 feature_193 (0.006086)\n",
      "28. feature 257 feature_257 (0.005933)\n",
      "29. feature 167 feature_167 (0.005811)\n",
      "30. feature 165 feature_165 (0.005739)\n",
      "31. feature 160 feature_160 (0.005484)\n",
      "32. feature 307 feature_307 (0.005398)\n",
      "33. feature 59 feature_59 (0.005187)\n",
      "34. feature 88 feature_88 (0.005170)\n",
      "35. feature 185 feature_185 (0.005161)\n",
      "36. feature 320 feature_320 (0.005128)\n",
      "37. feature 113 feature_113 (0.005115)\n",
      "38. feature 39 feature_39 (0.005095)\n",
      "39. feature 133 feature_133 (0.005030)\n",
      "40. feature 213 feature_213 (0.005029)\n",
      "41. feature 304 feature_304 (0.004884)\n",
      "42. feature 155 feature_155 (0.004742)\n",
      "43. feature 308 feature_308 (0.004684)\n",
      "44. feature 54 feature_54 (0.004648)\n",
      "45. feature 138 feature_138 (0.004608)\n",
      "46. feature 273 feature_273 (0.004573)\n",
      "47. feature 18 feature_18 (0.004549)\n",
      "48. feature 218 feature_218 (0.004543)\n",
      "49. feature 333 feature_333 (0.004478)\n",
      "50. feature 324 feature_324 (0.004430)\n",
      "51. feature 137 feature_137 (0.004417)\n",
      "52. feature 255 feature_255 (0.004359)\n",
      "53. feature 96 feature_96 (0.004308)\n",
      "54. feature 53 feature_53 (0.004297)\n",
      "55. feature 179 feature_179 (0.004240)\n",
      "56. feature 180 feature_180 (0.004195)\n",
      "57. feature 143 feature_143 (0.004186)\n",
      "58. feature 216 feature_216 (0.004130)\n",
      "59. feature 272 feature_272 (0.004127)\n",
      "60. feature 238 feature_238 (0.004126)\n",
      "61. feature 10 feature_10 (0.004113)\n",
      "62. feature 126 feature_126 (0.004112)\n",
      "63. feature 94 feature_94 (0.004090)\n",
      "64. feature 323 feature_323 (0.004080)\n",
      "65. feature 91 feature_91 (0.004077)\n",
      "66. feature 176 feature_176 (0.003994)\n",
      "67. feature 192 feature_192 (0.003986)\n",
      "68. feature 172 feature_172 (0.003952)\n",
      "69. feature 315 feature_315 (0.003912)\n",
      "70. feature 28 feature_28 (0.003892)\n",
      "71. feature 240 feature_240 (0.003793)\n",
      "72. feature 271 feature_271 (0.003769)\n",
      "73. feature 107 feature_107 (0.003752)\n",
      "74. feature 222 feature_222 (0.003732)\n",
      "75. feature 116 feature_116 (0.003706)\n",
      "76. feature 141 feature_141 (0.003692)\n",
      "77. feature 310 feature_310 (0.003643)\n",
      "78. feature 169 feature_169 (0.003633)\n",
      "79. feature 156 feature_156 (0.003615)\n",
      "80. feature 30 feature_30 (0.003600)\n",
      "81. feature 81 feature_81 (0.003583)\n",
      "82. feature 309 feature_309 (0.003556)\n",
      "83. feature 292 feature_292 (0.003444)\n",
      "84. feature 303 feature_303 (0.003408)\n",
      "85. feature 254 feature_254 (0.003397)\n",
      "86. feature 43 feature_43 (0.003386)\n",
      "87. feature 86 feature_86 (0.003379)\n",
      "88. feature 84 feature_84 (0.003370)\n",
      "89. feature 246 feature_246 (0.003343)\n",
      "90. feature 66 feature_66 (0.003329)\n",
      "91. feature 314 feature_314 (0.003323)\n",
      "92. feature 274 feature_274 (0.003313)\n",
      "93. feature 229 feature_229 (0.003293)\n",
      "94. feature 227 feature_227 (0.003277)\n",
      "95. feature 252 feature_252 (0.003274)\n",
      "96. feature 259 feature_259 (0.003247)\n",
      "97. feature 99 feature_99 (0.003221)\n",
      "98. feature 199 feature_199 (0.003203)\n",
      "99. feature 112 feature_112 (0.003201)\n",
      "100. feature 200 feature_200 (0.003196)\n",
      "(64, 200)\n",
      "(64, 336)\n",
      "64\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -0.003196   0.016818   0.037713   0.044312  -0.020322   0.028315   \n",
      "1   0.053181   0.046687   0.042593  -0.033030  -0.074276   0.034757   \n",
      "2   0.033206   0.017512  -0.025823  -0.054084  -0.024345   0.042596   \n",
      "3  -0.025786   0.078076  -0.054355  -0.028245  -0.052873   0.034688   \n",
      "4  -0.029932   0.062247  -0.080130   0.043698   0.071922  -0.058681   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_326  feature_327  \\\n",
      "0  -0.069711   0.035199   0.033197   0.013107  ...     0.050233     0.019413   \n",
      "1   0.087899  -0.046566  -0.036739  -0.030427  ...     0.015559     0.008920   \n",
      "2   0.016318  -0.003803  -0.024744  -0.077192  ...     0.069301     0.027037   \n",
      "3  -0.016117   0.052303   0.021985  -0.068788  ...     0.023310    -0.007083   \n",
      "4   0.062502  -0.033216  -0.038009  -0.086980  ...     0.043495     0.027045   \n",
      "\n",
      "   feature_328  feature_329  feature_330  feature_331  feature_332  \\\n",
      "0     0.029348     0.023202     0.042520     0.019367     0.004782   \n",
      "1     0.024316     0.042281     0.033839    -0.056984     0.191080   \n",
      "2     0.034130     0.023389     0.049297     0.034148     0.050799   \n",
      "3     0.031607     0.022231     0.026331     0.032082     0.213731   \n",
      "4     0.031489     0.019121     0.049694     0.118842     0.128444   \n",
      "\n",
      "   feature_333  feature_334  feature_335  \n",
      "0     0.025864    -0.023476     0.022704  \n",
      "1    -0.054130     0.037036     0.025750  \n",
      "2     0.049187    -0.042954     0.025952  \n",
      "3    -0.018759     0.221059     0.032400  \n",
      "4     0.402873    -0.062313     0.034587  \n",
      "\n",
      "[5 rows x 336 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature importance ranking\n",
      "1. feature 118 feature_118 (0.013843)\n",
      "2. feature 147 feature_147 (0.012683)\n",
      "3. feature 24 feature_24 (0.011028)\n",
      "4. feature 125 feature_125 (0.010660)\n",
      "5. feature 102 feature_102 (0.010611)\n",
      "6. feature 236 feature_236 (0.009529)\n",
      "7. feature 189 feature_189 (0.009009)\n",
      "8. feature 151 feature_151 (0.008997)\n",
      "9. feature 38 feature_38 (0.008652)\n",
      "10. feature 14 feature_14 (0.008348)\n",
      "11. feature 228 feature_228 (0.008306)\n",
      "12. feature 262 feature_262 (0.008297)\n",
      "13. feature 167 feature_167 (0.008238)\n",
      "14. feature 19 feature_19 (0.007797)\n",
      "15. feature 62 feature_62 (0.007642)\n",
      "16. feature 113 feature_113 (0.007500)\n",
      "17. feature 48 feature_48 (0.007442)\n",
      "18. feature 165 feature_165 (0.007163)\n",
      "19. feature 225 feature_225 (0.006835)\n",
      "20. feature 311 feature_311 (0.006765)\n",
      "21. feature 141 feature_141 (0.006711)\n",
      "22. feature 144 feature_144 (0.006288)\n",
      "23. feature 158 feature_158 (0.006281)\n",
      "24. feature 76 feature_76 (0.006272)\n",
      "25. feature 53 feature_53 (0.006223)\n",
      "26. feature 154 feature_154 (0.006169)\n",
      "27. feature 172 feature_172 (0.006129)\n",
      "28. feature 332 feature_332 (0.005696)\n",
      "29. feature 211 feature_211 (0.005677)\n",
      "30. feature 99 feature_99 (0.005520)\n",
      "31. feature 169 feature_169 (0.005500)\n",
      "32. feature 95 feature_95 (0.005374)\n",
      "33. feature 18 feature_18 (0.005349)\n",
      "34. feature 308 feature_308 (0.005276)\n",
      "35. feature 88 feature_88 (0.005273)\n",
      "36. feature 320 feature_320 (0.005127)\n",
      "37. feature 229 feature_229 (0.005090)\n",
      "38. feature 333 feature_333 (0.005055)\n",
      "39. feature 222 feature_222 (0.004959)\n",
      "40. feature 193 feature_193 (0.004903)\n",
      "41. feature 192 feature_192 (0.004896)\n",
      "42. feature 96 feature_96 (0.004808)\n",
      "43. feature 304 feature_304 (0.004784)\n",
      "44. feature 273 feature_273 (0.004749)\n",
      "45. feature 226 feature_226 (0.004711)\n",
      "46. feature 30 feature_30 (0.004588)\n",
      "47. feature 94 feature_94 (0.004477)\n",
      "48. feature 91 feature_91 (0.004324)\n",
      "49. feature 97 feature_97 (0.004321)\n",
      "50. feature 61 feature_61 (0.004311)\n",
      "51. feature 330 feature_330 (0.004309)\n",
      "52. feature 153 feature_153 (0.004307)\n",
      "53. feature 160 feature_160 (0.004297)\n",
      "54. feature 217 feature_217 (0.004281)\n",
      "55. feature 92 feature_92 (0.004271)\n",
      "56. feature 54 feature_54 (0.004173)\n",
      "57. feature 216 feature_216 (0.004087)\n",
      "58. feature 257 feature_257 (0.004084)\n",
      "59. feature 296 feature_296 (0.004068)\n",
      "60. feature 176 feature_176 (0.004068)\n",
      "61. feature 133 feature_133 (0.004058)\n",
      "62. feature 137 feature_137 (0.004033)\n",
      "63. feature 307 feature_307 (0.003971)\n",
      "64. feature 72 feature_72 (0.003949)\n",
      "65. feature 146 feature_146 (0.003885)\n",
      "66. feature 263 feature_263 (0.003846)\n",
      "67. feature 10 feature_10 (0.003820)\n",
      "68. feature 293 feature_293 (0.003816)\n",
      "69. feature 276 feature_276 (0.003792)\n",
      "70. feature 33 feature_33 (0.003763)\n",
      "71. feature 255 feature_255 (0.003734)\n",
      "72. feature 111 feature_111 (0.003544)\n",
      "73. feature 9 feature_9 (0.003521)\n",
      "74. feature 2 feature_2 (0.003516)\n",
      "75. feature 280 feature_280 (0.003513)\n",
      "76. feature 179 feature_179 (0.003467)\n",
      "77. feature 7 feature_7 (0.003450)\n",
      "78. feature 128 feature_128 (0.003433)\n",
      "79. feature 194 feature_194 (0.003433)\n",
      "80. feature 149 feature_149 (0.003422)\n",
      "81. feature 198 feature_198 (0.003410)\n",
      "82. feature 143 feature_143 (0.003402)\n",
      "83. feature 122 feature_122 (0.003400)\n",
      "84. feature 281 feature_281 (0.003391)\n",
      "85. feature 227 feature_227 (0.003390)\n",
      "86. feature 269 feature_269 (0.003349)\n",
      "87. feature 31 feature_31 (0.003342)\n",
      "88. feature 288 feature_288 (0.003328)\n",
      "89. feature 244 feature_244 (0.003316)\n",
      "90. feature 246 feature_246 (0.003304)\n",
      "91. feature 63 feature_63 (0.003282)\n",
      "92. feature 206 feature_206 (0.003277)\n",
      "93. feature 251 feature_251 (0.003265)\n",
      "94. feature 16 feature_16 (0.003248)\n",
      "95. feature 46 feature_46 (0.003225)\n",
      "96. feature 166 feature_166 (0.003219)\n",
      "97. feature 303 feature_303 (0.003216)\n",
      "98. feature 249 feature_249 (0.003209)\n",
      "99. feature 260 feature_260 (0.003207)\n",
      "100. feature 26 feature_26 (0.003184)\n",
      "(64, 250)\n",
      "(64, 336)\n",
      "64\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -0.003196   0.016818   0.037713   0.044312  -0.020322   0.028315   \n",
      "1   0.053181   0.046687   0.042593  -0.033030  -0.074276   0.034757   \n",
      "2   0.033206   0.017512  -0.025823  -0.054084  -0.024345   0.042596   \n",
      "3  -0.025786   0.078076  -0.054355  -0.028245  -0.052873   0.034688   \n",
      "4  -0.029932   0.062247  -0.080130   0.043698   0.071922  -0.058681   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_326  feature_327  \\\n",
      "0  -0.069711   0.035199   0.033197   0.013107  ...     0.050233     0.019413   \n",
      "1   0.087899  -0.046566  -0.036739  -0.030427  ...     0.015559     0.008920   \n",
      "2   0.016318  -0.003803  -0.024744  -0.077192  ...     0.069301     0.027037   \n",
      "3  -0.016117   0.052303   0.021985  -0.068788  ...     0.023310    -0.007083   \n",
      "4   0.062502  -0.033216  -0.038009  -0.086980  ...     0.043495     0.027045   \n",
      "\n",
      "   feature_328  feature_329  feature_330  feature_331  feature_332  \\\n",
      "0     0.029348     0.023202     0.042520     0.019367     0.004782   \n",
      "1     0.024316     0.042281     0.033839    -0.056984     0.191080   \n",
      "2     0.034130     0.023389     0.049297     0.034148     0.050799   \n",
      "3     0.031607     0.022231     0.026331     0.032082     0.213731   \n",
      "4     0.031489     0.019121     0.049694     0.118842     0.128444   \n",
      "\n",
      "   feature_333  feature_334  feature_335  \n",
      "0     0.025864    -0.023476     0.022704  \n",
      "1    -0.054130     0.037036     0.025750  \n",
      "2     0.049187    -0.042954     0.025952  \n",
      "3    -0.018759     0.221059     0.032400  \n",
      "4     0.402873    -0.062313     0.034587  \n",
      "\n",
      "[5 rows x 336 columns]\n",
      "\n",
      "feature importance ranking\n",
      "1. feature 118 feature_118 (0.016972)\n",
      "2. feature 147 feature_147 (0.014529)\n",
      "3. feature 151 feature_151 (0.013098)\n",
      "4. feature 102 feature_102 (0.012761)\n",
      "5. feature 24 feature_24 (0.010517)\n",
      "6. feature 125 feature_125 (0.009863)\n",
      "7. feature 236 feature_236 (0.009471)\n",
      "8. feature 62 feature_62 (0.009123)\n",
      "9. feature 95 feature_95 (0.008734)\n",
      "10. feature 167 feature_167 (0.008128)\n",
      "11. feature 225 feature_225 (0.008071)\n",
      "12. feature 14 feature_14 (0.007955)\n",
      "13. feature 113 feature_113 (0.007870)\n",
      "14. feature 38 feature_38 (0.007385)\n",
      "15. feature 332 feature_332 (0.007320)\n",
      "16. feature 154 feature_154 (0.007314)\n",
      "17. feature 189 feature_189 (0.007159)\n",
      "18. feature 304 feature_304 (0.006442)\n",
      "19. feature 311 feature_311 (0.006379)\n",
      "20. feature 96 feature_96 (0.006276)\n",
      "21. feature 144 feature_144 (0.006200)\n",
      "22. feature 48 feature_48 (0.006045)\n",
      "23. feature 226 feature_226 (0.006001)\n",
      "24. feature 76 feature_76 (0.006001)\n",
      "25. feature 308 feature_308 (0.005754)\n",
      "26. feature 228 feature_228 (0.005719)\n",
      "27. feature 216 feature_216 (0.005506)\n",
      "28. feature 141 feature_141 (0.005375)\n",
      "29. feature 158 feature_158 (0.005303)\n",
      "30. feature 92 feature_92 (0.005264)\n",
      "31. feature 53 feature_53 (0.005230)\n",
      "32. feature 91 feature_91 (0.005113)\n",
      "33. feature 211 feature_211 (0.005062)\n",
      "34. feature 176 feature_176 (0.005029)\n",
      "35. feature 172 feature_172 (0.005027)\n",
      "36. feature 320 feature_320 (0.004998)\n",
      "37. feature 19 feature_19 (0.004928)\n",
      "38. feature 199 feature_199 (0.004879)\n",
      "39. feature 307 feature_307 (0.004810)\n",
      "40. feature 88 feature_88 (0.004791)\n",
      "41. feature 198 feature_198 (0.004789)\n",
      "42. feature 160 feature_160 (0.004730)\n",
      "43. feature 276 feature_276 (0.004709)\n",
      "44. feature 165 feature_165 (0.004656)\n",
      "45. feature 29 feature_29 (0.004618)\n",
      "46. feature 169 feature_169 (0.004530)\n",
      "47. feature 229 feature_229 (0.004525)\n",
      "48. feature 133 feature_133 (0.004449)\n",
      "49. feature 31 feature_31 (0.004423)\n",
      "50. feature 259 feature_259 (0.004327)\n",
      "51. feature 192 feature_192 (0.004290)\n",
      "52. feature 246 feature_246 (0.004279)\n",
      "53. feature 179 feature_179 (0.004239)\n",
      "54. feature 49 feature_49 (0.004212)\n",
      "55. feature 200 feature_200 (0.004098)\n",
      "56. feature 288 feature_288 (0.004024)\n",
      "57. feature 18 feature_18 (0.003998)\n",
      "58. feature 182 feature_182 (0.003985)\n",
      "59. feature 240 feature_240 (0.003955)\n",
      "60. feature 143 feature_143 (0.003907)\n",
      "61. feature 20 feature_20 (0.003874)\n",
      "62. feature 262 feature_262 (0.003867)\n",
      "63. feature 193 feature_193 (0.003865)\n",
      "64. feature 56 feature_56 (0.003847)\n",
      "65. feature 255 feature_255 (0.003831)\n",
      "66. feature 291 feature_291 (0.003809)\n",
      "67. feature 9 feature_9 (0.003790)\n",
      "68. feature 333 feature_333 (0.003773)\n",
      "69. feature 84 feature_84 (0.003770)\n",
      "70. feature 328 feature_328 (0.003764)\n",
      "71. feature 89 feature_89 (0.003763)\n",
      "72. feature 94 feature_94 (0.003725)\n",
      "73. feature 257 feature_257 (0.003718)\n",
      "74. feature 306 feature_306 (0.003706)\n",
      "75. feature 260 feature_260 (0.003678)\n",
      "76. feature 46 feature_46 (0.003675)\n",
      "77. feature 4 feature_4 (0.003657)\n",
      "78. feature 185 feature_185 (0.003558)\n",
      "79. feature 170 feature_170 (0.003541)\n",
      "80. feature 218 feature_218 (0.003540)\n",
      "81. feature 2 feature_2 (0.003537)\n",
      "82. feature 138 feature_138 (0.003529)\n",
      "83. feature 207 feature_207 (0.003514)\n",
      "84. feature 181 feature_181 (0.003486)\n",
      "85. feature 264 feature_264 (0.003445)\n",
      "86. feature 146 feature_146 (0.003430)\n",
      "87. feature 63 feature_63 (0.003423)\n",
      "88. feature 323 feature_323 (0.003385)\n",
      "89. feature 116 feature_116 (0.003364)\n",
      "90. feature 43 feature_43 (0.003364)\n",
      "91. feature 149 feature_149 (0.003360)\n",
      "92. feature 283 feature_283 (0.003339)\n",
      "93. feature 287 feature_287 (0.003301)\n",
      "94. feature 263 feature_263 (0.003288)\n",
      "95. feature 61 feature_61 (0.003285)\n",
      "96. feature 272 feature_272 (0.003264)\n",
      "97. feature 44 feature_44 (0.003263)\n",
      "98. feature 191 feature_191 (0.003249)\n",
      "99. feature 249 feature_249 (0.003240)\n",
      "100. feature 99 feature_99 (0.003238)\n",
      "(64, 300)\n",
      "(64, 672)\n",
      "64\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -0.038135  -0.021344   0.050218  -0.045803  -0.033146   0.029607   \n",
      "1  -0.017397   0.003196   0.025299  -0.049143   0.020379   0.019599   \n",
      "2   0.008102  -0.026595  -0.038854   0.023067   0.000956  -0.032514   \n",
      "3   0.091391  -0.034410  -0.046202   0.010334  -0.039960   0.047707   \n",
      "4  -0.016237   0.023195   0.043193   0.013891   0.019974  -0.040611   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_662  feature_663  \\\n",
      "0   0.031293  -0.003199  -0.046154   0.029457  ...     0.021250     0.023258   \n",
      "1  -0.024797  -0.018282  -0.032134  -0.021385  ...    -0.085980     0.035986   \n",
      "2   0.024831   0.014583   0.015340  -0.048966  ...     0.034363     0.025496   \n",
      "3   0.017128   0.020622  -0.065581  -0.036677  ...    -0.007478     0.017845   \n",
      "4  -0.022273   0.096411  -0.062761  -0.067945  ...     0.028631     0.015119   \n",
      "\n",
      "   feature_664  feature_665  feature_666  feature_667  feature_668  \\\n",
      "0     0.024367     0.051106     0.024030     0.047877     0.068056   \n",
      "1     0.032514     0.034748    -0.036323     0.035792     0.082239   \n",
      "2     0.019904     0.025447     0.013342     0.008534     0.055924   \n",
      "3     0.016041     0.022515     0.015971    -0.019444    -0.009630   \n",
      "4     0.034319     0.034803     0.031954    -0.052447     0.023465   \n",
      "\n",
      "   feature_669  feature_670  feature_671  \n",
      "0    -0.088991     0.077928     0.023265  \n",
      "1     0.034407     0.104235     0.033394  \n",
      "2    -0.023555     0.022802     0.025472  \n",
      "3     0.016138     0.025911     0.012235  \n",
      "4     0.014136     0.021291     0.015567  \n",
      "\n",
      "[5 rows x 672 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature importance ranking\n",
      "1. feature 72 feature_72 (0.015746)\n",
      "2. feature 281 feature_281 (0.015251)\n",
      "3. feature 35 feature_35 (0.009751)\n",
      "4. feature 470 feature_470 (0.008966)\n",
      "5. feature 124 feature_124 (0.008231)\n",
      "6. feature 556 feature_556 (0.007224)\n",
      "7. feature 44 feature_44 (0.005640)\n",
      "8. feature 326 feature_326 (0.005409)\n",
      "9. feature 392 feature_392 (0.005325)\n",
      "10. feature 47 feature_47 (0.005307)\n",
      "11. feature 271 feature_271 (0.005205)\n",
      "12. feature 88 feature_88 (0.004668)\n",
      "13. feature 322 feature_322 (0.004656)\n",
      "14. feature 155 feature_155 (0.004534)\n",
      "15. feature 511 feature_511 (0.004504)\n",
      "16. feature 331 feature_331 (0.004437)\n",
      "17. feature 651 feature_651 (0.004337)\n",
      "18. feature 137 feature_137 (0.004315)\n",
      "19. feature 538 feature_538 (0.004231)\n",
      "20. feature 273 feature_273 (0.004226)\n",
      "21. feature 653 feature_653 (0.004058)\n",
      "22. feature 525 feature_525 (0.004019)\n",
      "23. feature 134 feature_134 (0.004011)\n",
      "24. feature 74 feature_74 (0.004000)\n",
      "25. feature 540 feature_540 (0.003995)\n",
      "26. feature 16 feature_16 (0.003944)\n",
      "27. feature 294 feature_294 (0.003940)\n",
      "28. feature 489 feature_489 (0.003921)\n",
      "29. feature 671 feature_671 (0.003888)\n",
      "30. feature 599 feature_599 (0.003867)\n",
      "31. feature 350 feature_350 (0.003838)\n",
      "32. feature 285 feature_285 (0.003822)\n",
      "33. feature 158 feature_158 (0.003752)\n",
      "34. feature 29 feature_29 (0.003664)\n",
      "35. feature 476 feature_476 (0.003657)\n",
      "36. feature 40 feature_40 (0.003642)\n",
      "37. feature 7 feature_7 (0.003598)\n",
      "38. feature 660 feature_660 (0.003591)\n",
      "39. feature 346 feature_346 (0.003488)\n",
      "40. feature 595 feature_595 (0.003486)\n",
      "41. feature 126 feature_126 (0.003457)\n",
      "42. feature 89 feature_89 (0.003452)\n",
      "43. feature 144 feature_144 (0.003443)\n",
      "44. feature 181 feature_181 (0.003406)\n",
      "45. feature 212 feature_212 (0.003391)\n",
      "46. feature 304 feature_304 (0.003372)\n",
      "47. feature 49 feature_49 (0.003358)\n",
      "48. feature 41 feature_41 (0.003278)\n",
      "49. feature 546 feature_546 (0.003250)\n",
      "50. feature 293 feature_293 (0.003243)\n",
      "51. feature 112 feature_112 (0.003230)\n",
      "52. feature 202 feature_202 (0.003230)\n",
      "53. feature 353 feature_353 (0.003193)\n",
      "54. feature 316 feature_316 (0.003187)\n",
      "55. feature 642 feature_642 (0.003113)\n",
      "56. feature 211 feature_211 (0.003095)\n",
      "57. feature 171 feature_171 (0.003038)\n",
      "58. feature 504 feature_504 (0.003021)\n",
      "59. feature 466 feature_466 (0.003016)\n",
      "60. feature 51 feature_51 (0.002977)\n",
      "61. feature 307 feature_307 (0.002884)\n",
      "62. feature 605 feature_605 (0.002883)\n",
      "63. feature 468 feature_468 (0.002883)\n",
      "64. feature 372 feature_372 (0.002862)\n",
      "65. feature 143 feature_143 (0.002858)\n",
      "66. feature 67 feature_67 (0.002850)\n",
      "67. feature 454 feature_454 (0.002806)\n",
      "68. feature 15 feature_15 (0.002776)\n",
      "69. feature 26 feature_26 (0.002773)\n",
      "70. feature 129 feature_129 (0.002768)\n",
      "71. feature 204 feature_204 (0.002730)\n",
      "72. feature 222 feature_222 (0.002716)\n",
      "73. feature 348 feature_348 (0.002716)\n",
      "74. feature 442 feature_442 (0.002710)\n",
      "75. feature 305 feature_305 (0.002677)\n",
      "76. feature 19 feature_19 (0.002667)\n",
      "77. feature 231 feature_231 (0.002620)\n",
      "78. feature 20 feature_20 (0.002574)\n",
      "79. feature 60 feature_60 (0.002569)\n",
      "80. feature 17 feature_17 (0.002540)\n",
      "81. feature 457 feature_457 (0.002539)\n",
      "82. feature 138 feature_138 (0.002530)\n",
      "83. feature 315 feature_315 (0.002528)\n",
      "84. feature 668 feature_668 (0.002504)\n",
      "85. feature 596 feature_596 (0.002502)\n",
      "86. feature 357 feature_357 (0.002461)\n",
      "87. feature 641 feature_641 (0.002429)\n",
      "88. feature 336 feature_336 (0.002426)\n",
      "89. feature 663 feature_663 (0.002422)\n",
      "90. feature 218 feature_218 (0.002421)\n",
      "91. feature 386 feature_386 (0.002406)\n",
      "92. feature 1 feature_1 (0.002406)\n",
      "93. feature 189 feature_189 (0.002391)\n",
      "94. feature 408 feature_408 (0.002389)\n",
      "95. feature 490 feature_490 (0.002379)\n",
      "96. feature 475 feature_475 (0.002378)\n",
      "97. feature 205 feature_205 (0.002371)\n",
      "98. feature 617 feature_617 (0.002350)\n",
      "99. feature 319 feature_319 (0.002336)\n",
      "100. feature 103 feature_103 (0.002333)\n",
      "(64, 50)\n",
      "(64, 672)\n",
      "64\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -0.038135  -0.021344   0.050218  -0.045803  -0.033146   0.029607   \n",
      "1  -0.017397   0.003196   0.025299  -0.049143   0.020379   0.019599   \n",
      "2   0.008102  -0.026595  -0.038854   0.023067   0.000956  -0.032514   \n",
      "3   0.091391  -0.034410  -0.046202   0.010334  -0.039960   0.047707   \n",
      "4  -0.016237   0.023195   0.043193   0.013891   0.019974  -0.040611   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_662  feature_663  \\\n",
      "0   0.031293  -0.003199  -0.046154   0.029457  ...     0.021250     0.023258   \n",
      "1  -0.024797  -0.018282  -0.032134  -0.021385  ...    -0.085980     0.035986   \n",
      "2   0.024831   0.014583   0.015340  -0.048966  ...     0.034363     0.025496   \n",
      "3   0.017128   0.020622  -0.065581  -0.036677  ...    -0.007478     0.017845   \n",
      "4  -0.022273   0.096411  -0.062761  -0.067945  ...     0.028631     0.015119   \n",
      "\n",
      "   feature_664  feature_665  feature_666  feature_667  feature_668  \\\n",
      "0     0.024367     0.051106     0.024030     0.047877     0.068056   \n",
      "1     0.032514     0.034748    -0.036323     0.035792     0.082239   \n",
      "2     0.019904     0.025447     0.013342     0.008534     0.055924   \n",
      "3     0.016041     0.022515     0.015971    -0.019444    -0.009630   \n",
      "4     0.034319     0.034803     0.031954    -0.052447     0.023465   \n",
      "\n",
      "   feature_669  feature_670  feature_671  \n",
      "0    -0.088991     0.077928     0.023265  \n",
      "1     0.034407     0.104235     0.033394  \n",
      "2    -0.023555     0.022802     0.025472  \n",
      "3     0.016138     0.025911     0.012235  \n",
      "4     0.014136     0.021291     0.015567  \n",
      "\n",
      "[5 rows x 672 columns]\n",
      "\n",
      "feature importance ranking\n",
      "1. feature 72 feature_72 (0.014960)\n",
      "2. feature 281 feature_281 (0.011530)\n",
      "3. feature 556 feature_556 (0.008126)\n",
      "4. feature 124 feature_124 (0.007836)\n",
      "5. feature 538 feature_538 (0.007754)\n",
      "6. feature 470 feature_470 (0.007508)\n",
      "7. feature 392 feature_392 (0.006177)\n",
      "8. feature 653 feature_653 (0.005653)\n",
      "9. feature 671 feature_671 (0.005609)\n",
      "10. feature 525 feature_525 (0.005519)\n",
      "11. feature 326 feature_326 (0.005506)\n",
      "12. feature 47 feature_47 (0.005400)\n",
      "13. feature 171 feature_171 (0.005390)\n",
      "14. feature 35 feature_35 (0.005139)\n",
      "15. feature 293 feature_293 (0.005134)\n",
      "16. feature 353 feature_353 (0.005064)\n",
      "17. feature 294 feature_294 (0.004879)\n",
      "18. feature 651 feature_651 (0.004875)\n",
      "19. feature 599 feature_599 (0.004627)\n",
      "20. feature 546 feature_546 (0.004581)\n",
      "21. feature 41 feature_41 (0.004177)\n",
      "22. feature 126 feature_126 (0.004152)\n",
      "23. feature 44 feature_44 (0.004133)\n",
      "24. feature 40 feature_40 (0.004075)\n",
      "25. feature 322 feature_322 (0.004027)\n",
      "26. feature 19 feature_19 (0.004015)\n",
      "27. feature 505 feature_505 (0.003964)\n",
      "28. feature 625 feature_625 (0.003894)\n",
      "29. feature 7 feature_7 (0.003836)\n",
      "30. feature 29 feature_29 (0.003826)\n",
      "31. feature 346 feature_346 (0.003807)\n",
      "32. feature 49 feature_49 (0.003771)\n",
      "33. feature 67 feature_67 (0.003736)\n",
      "34. feature 596 feature_596 (0.003733)\n",
      "35. feature 660 feature_660 (0.003702)\n",
      "36. feature 246 feature_246 (0.003627)\n",
      "37. feature 476 feature_476 (0.003604)\n",
      "38. feature 144 feature_144 (0.003517)\n",
      "39. feature 143 feature_143 (0.003466)\n",
      "40. feature 222 feature_222 (0.003458)\n",
      "41. feature 211 feature_211 (0.003434)\n",
      "42. feature 88 feature_88 (0.003418)\n",
      "43. feature 475 feature_475 (0.003415)\n",
      "44. feature 642 feature_642 (0.003360)\n",
      "45. feature 1 feature_1 (0.003352)\n",
      "46. feature 129 feature_129 (0.003346)\n",
      "47. feature 51 feature_51 (0.003317)\n",
      "48. feature 181 feature_181 (0.003230)\n",
      "49. feature 16 feature_16 (0.003225)\n",
      "50. feature 74 feature_74 (0.003196)\n",
      "51. feature 595 feature_595 (0.003192)\n",
      "52. feature 501 feature_501 (0.003190)\n",
      "53. feature 104 feature_104 (0.003168)\n",
      "54. feature 547 feature_547 (0.003122)\n",
      "55. feature 446 feature_446 (0.003118)\n",
      "56. feature 442 feature_442 (0.003108)\n",
      "57. feature 305 feature_305 (0.003097)\n",
      "58. feature 486 feature_486 (0.003091)\n",
      "59. feature 212 feature_212 (0.003057)\n",
      "60. feature 231 feature_231 (0.002994)\n",
      "61. feature 351 feature_351 (0.002959)\n",
      "62. feature 148 feature_148 (0.002931)\n",
      "63. feature 454 feature_454 (0.002892)\n",
      "64. feature 325 feature_325 (0.002890)\n",
      "65. feature 331 feature_331 (0.002863)\n",
      "66. feature 489 feature_489 (0.002798)\n",
      "67. feature 300 feature_300 (0.002769)\n",
      "68. feature 413 feature_413 (0.002748)\n",
      "69. feature 563 feature_563 (0.002714)\n",
      "70. feature 614 feature_614 (0.002685)\n",
      "71. feature 158 feature_158 (0.002685)\n",
      "72. feature 319 feature_319 (0.002680)\n",
      "73. feature 316 feature_316 (0.002671)\n",
      "74. feature 532 feature_532 (0.002665)\n",
      "75. feature 128 feature_128 (0.002665)\n",
      "76. feature 107 feature_107 (0.002664)\n",
      "77. feature 271 feature_271 (0.002648)\n",
      "78. feature 663 feature_663 (0.002644)\n",
      "79. feature 357 feature_357 (0.002643)\n",
      "80. feature 11 feature_11 (0.002638)\n",
      "81. feature 303 feature_303 (0.002620)\n",
      "82. feature 466 feature_466 (0.002611)\n",
      "83. feature 372 feature_372 (0.002593)\n",
      "84. feature 336 feature_336 (0.002577)\n",
      "85. feature 290 feature_290 (0.002554)\n",
      "86. feature 333 feature_333 (0.002531)\n",
      "87. feature 339 feature_339 (0.002519)\n",
      "88. feature 533 feature_533 (0.002456)\n",
      "89. feature 360 feature_360 (0.002437)\n",
      "90. feature 307 feature_307 (0.002400)\n",
      "91. feature 8 feature_8 (0.002370)\n",
      "92. feature 474 feature_474 (0.002369)\n",
      "93. feature 504 feature_504 (0.002351)\n",
      "94. feature 511 feature_511 (0.002342)\n",
      "95. feature 327 feature_327 (0.002339)\n",
      "96. feature 50 feature_50 (0.002338)\n",
      "97. feature 76 feature_76 (0.002296)\n",
      "98. feature 149 feature_149 (0.002293)\n",
      "99. feature 34 feature_34 (0.002290)\n",
      "100. feature 448 feature_448 (0.002286)\n",
      "(64, 100)\n",
      "(64, 672)\n",
      "64\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -0.038135  -0.021344   0.050218  -0.045803  -0.033146   0.029607   \n",
      "1  -0.017397   0.003196   0.025299  -0.049143   0.020379   0.019599   \n",
      "2   0.008102  -0.026595  -0.038854   0.023067   0.000956  -0.032514   \n",
      "3   0.091391  -0.034410  -0.046202   0.010334  -0.039960   0.047707   \n",
      "4  -0.016237   0.023195   0.043193   0.013891   0.019974  -0.040611   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_662  feature_663  \\\n",
      "0   0.031293  -0.003199  -0.046154   0.029457  ...     0.021250     0.023258   \n",
      "1  -0.024797  -0.018282  -0.032134  -0.021385  ...    -0.085980     0.035986   \n",
      "2   0.024831   0.014583   0.015340  -0.048966  ...     0.034363     0.025496   \n",
      "3   0.017128   0.020622  -0.065581  -0.036677  ...    -0.007478     0.017845   \n",
      "4  -0.022273   0.096411  -0.062761  -0.067945  ...     0.028631     0.015119   \n",
      "\n",
      "   feature_664  feature_665  feature_666  feature_667  feature_668  \\\n",
      "0     0.024367     0.051106     0.024030     0.047877     0.068056   \n",
      "1     0.032514     0.034748    -0.036323     0.035792     0.082239   \n",
      "2     0.019904     0.025447     0.013342     0.008534     0.055924   \n",
      "3     0.016041     0.022515     0.015971    -0.019444    -0.009630   \n",
      "4     0.034319     0.034803     0.031954    -0.052447     0.023465   \n",
      "\n",
      "   feature_669  feature_670  feature_671  \n",
      "0    -0.088991     0.077928     0.023265  \n",
      "1     0.034407     0.104235     0.033394  \n",
      "2    -0.023555     0.022802     0.025472  \n",
      "3     0.016138     0.025911     0.012235  \n",
      "4     0.014136     0.021291     0.015567  \n",
      "\n",
      "[5 rows x 672 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature importance ranking\n",
      "1. feature 281 feature_281 (0.017778)\n",
      "2. feature 72 feature_72 (0.014769)\n",
      "3. feature 470 feature_470 (0.010100)\n",
      "4. feature 556 feature_556 (0.008906)\n",
      "5. feature 124 feature_124 (0.007678)\n",
      "6. feature 346 feature_346 (0.006384)\n",
      "7. feature 322 feature_322 (0.006097)\n",
      "8. feature 653 feature_653 (0.005996)\n",
      "9. feature 525 feature_525 (0.005758)\n",
      "10. feature 671 feature_671 (0.005529)\n",
      "11. feature 326 feature_326 (0.005083)\n",
      "12. feature 40 feature_40 (0.004925)\n",
      "13. feature 44 feature_44 (0.004892)\n",
      "14. feature 294 feature_294 (0.004860)\n",
      "15. feature 666 feature_666 (0.004859)\n",
      "16. feature 538 feature_538 (0.004689)\n",
      "17. feature 476 feature_476 (0.004539)\n",
      "18. feature 392 feature_392 (0.004513)\n",
      "19. feature 16 feature_16 (0.004344)\n",
      "20. feature 74 feature_74 (0.004210)\n",
      "21. feature 155 feature_155 (0.004207)\n",
      "22. feature 19 feature_19 (0.004201)\n",
      "23. feature 88 feature_88 (0.004188)\n",
      "24. feature 49 feature_49 (0.004150)\n",
      "25. feature 35 feature_35 (0.003967)\n",
      "26. feature 130 feature_130 (0.003899)\n",
      "27. feature 258 feature_258 (0.003894)\n",
      "28. feature 304 feature_304 (0.003797)\n",
      "29. feature 47 feature_47 (0.003711)\n",
      "30. feature 540 feature_540 (0.003703)\n",
      "31. feature 107 feature_107 (0.003619)\n",
      "32. feature 29 feature_29 (0.003616)\n",
      "33. feature 660 feature_660 (0.003607)\n",
      "34. feature 273 feature_273 (0.003516)\n",
      "35. feature 350 feature_350 (0.003416)\n",
      "36. feature 129 feature_129 (0.003348)\n",
      "37. feature 466 feature_466 (0.003297)\n",
      "38. feature 594 feature_594 (0.003254)\n",
      "39. feature 67 feature_67 (0.003238)\n",
      "40. feature 387 feature_387 (0.003211)\n",
      "41. feature 547 feature_547 (0.003207)\n",
      "42. feature 625 feature_625 (0.003171)\n",
      "43. feature 41 feature_41 (0.003154)\n",
      "44. feature 138 feature_138 (0.003150)\n",
      "45. feature 651 feature_651 (0.003114)\n",
      "46. feature 331 feature_331 (0.003107)\n",
      "47. feature 144 feature_144 (0.003056)\n",
      "48. feature 11 feature_11 (0.003048)\n",
      "49. feature 226 feature_226 (0.003031)\n",
      "50. feature 357 feature_357 (0.003029)\n",
      "51. feature 642 feature_642 (0.003023)\n",
      "52. feature 595 feature_595 (0.003020)\n",
      "53. feature 323 feature_323 (0.003015)\n",
      "54. feature 609 feature_609 (0.002994)\n",
      "55. feature 205 feature_205 (0.002981)\n",
      "56. feature 386 feature_386 (0.002979)\n",
      "57. feature 396 feature_396 (0.002974)\n",
      "58. feature 297 feature_297 (0.002952)\n",
      "59. feature 468 feature_468 (0.002945)\n",
      "60. feature 546 feature_546 (0.002936)\n",
      "61. feature 211 feature_211 (0.002929)\n",
      "62. feature 458 feature_458 (0.002925)\n",
      "63. feature 505 feature_505 (0.002919)\n",
      "64. feature 252 feature_252 (0.002890)\n",
      "65. feature 194 feature_194 (0.002878)\n",
      "66. feature 171 feature_171 (0.002872)\n",
      "67. feature 504 feature_504 (0.002870)\n",
      "68. feature 7 feature_7 (0.002859)\n",
      "69. feature 565 feature_565 (0.002836)\n",
      "70. feature 34 feature_34 (0.002835)\n",
      "71. feature 599 feature_599 (0.002822)\n",
      "72. feature 413 feature_413 (0.002786)\n",
      "73. feature 511 feature_511 (0.002775)\n",
      "74. feature 493 feature_493 (0.002770)\n",
      "75. feature 300 feature_300 (0.002768)\n",
      "76. feature 51 feature_51 (0.002744)\n",
      "77. feature 218 feature_218 (0.002715)\n",
      "78. feature 339 feature_339 (0.002714)\n",
      "79. feature 143 feature_143 (0.002704)\n",
      "80. feature 518 feature_518 (0.002697)\n",
      "81. feature 12 feature_12 (0.002687)\n",
      "82. feature 442 feature_442 (0.002657)\n",
      "83. feature 615 feature_615 (0.002646)\n",
      "84. feature 605 feature_605 (0.002642)\n",
      "85. feature 27 feature_27 (0.002627)\n",
      "86. feature 181 feature_181 (0.002614)\n",
      "87. feature 663 feature_663 (0.002611)\n",
      "88. feature 313 feature_313 (0.002607)\n",
      "89. feature 46 feature_46 (0.002607)\n",
      "90. feature 372 feature_372 (0.002597)\n",
      "91. feature 454 feature_454 (0.002596)\n",
      "92. feature 202 feature_202 (0.002591)\n",
      "93. feature 633 feature_633 (0.002582)\n",
      "94. feature 348 feature_348 (0.002511)\n",
      "95. feature 446 feature_446 (0.002509)\n",
      "96. feature 222 feature_222 (0.002506)\n",
      "97. feature 231 feature_231 (0.002475)\n",
      "98. feature 360 feature_360 (0.002472)\n",
      "99. feature 655 feature_655 (0.002469)\n",
      "100. feature 75 feature_75 (0.002467)\n",
      "(64, 150)\n",
      "(64, 672)\n",
      "64\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -0.038135  -0.021344   0.050218  -0.045803  -0.033146   0.029607   \n",
      "1  -0.017397   0.003196   0.025299  -0.049143   0.020379   0.019599   \n",
      "2   0.008102  -0.026595  -0.038854   0.023067   0.000956  -0.032514   \n",
      "3   0.091391  -0.034410  -0.046202   0.010334  -0.039960   0.047707   \n",
      "4  -0.016237   0.023195   0.043193   0.013891   0.019974  -0.040611   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_662  feature_663  \\\n",
      "0   0.031293  -0.003199  -0.046154   0.029457  ...     0.021250     0.023258   \n",
      "1  -0.024797  -0.018282  -0.032134  -0.021385  ...    -0.085980     0.035986   \n",
      "2   0.024831   0.014583   0.015340  -0.048966  ...     0.034363     0.025496   \n",
      "3   0.017128   0.020622  -0.065581  -0.036677  ...    -0.007478     0.017845   \n",
      "4  -0.022273   0.096411  -0.062761  -0.067945  ...     0.028631     0.015119   \n",
      "\n",
      "   feature_664  feature_665  feature_666  feature_667  feature_668  \\\n",
      "0     0.024367     0.051106     0.024030     0.047877     0.068056   \n",
      "1     0.032514     0.034748    -0.036323     0.035792     0.082239   \n",
      "2     0.019904     0.025447     0.013342     0.008534     0.055924   \n",
      "3     0.016041     0.022515     0.015971    -0.019444    -0.009630   \n",
      "4     0.034319     0.034803     0.031954    -0.052447     0.023465   \n",
      "\n",
      "   feature_669  feature_670  feature_671  \n",
      "0    -0.088991     0.077928     0.023265  \n",
      "1     0.034407     0.104235     0.033394  \n",
      "2    -0.023555     0.022802     0.025472  \n",
      "3     0.016138     0.025911     0.012235  \n",
      "4     0.014136     0.021291     0.015567  \n",
      "\n",
      "[5 rows x 672 columns]\n",
      "\n",
      "feature importance ranking\n",
      "1. feature 72 feature_72 (0.013776)\n",
      "2. feature 281 feature_281 (0.011029)\n",
      "3. feature 470 feature_470 (0.009624)\n",
      "4. feature 556 feature_556 (0.008232)\n",
      "5. feature 671 feature_671 (0.007428)\n",
      "6. feature 47 feature_47 (0.007160)\n",
      "7. feature 124 feature_124 (0.006965)\n",
      "8. feature 511 feature_511 (0.005720)\n",
      "9. feature 222 feature_222 (0.005706)\n",
      "10. feature 651 feature_651 (0.005685)\n",
      "11. feature 67 feature_67 (0.005560)\n",
      "12. feature 392 feature_392 (0.005436)\n",
      "13. feature 44 feature_44 (0.005348)\n",
      "14. feature 322 feature_322 (0.005267)\n",
      "15. feature 35 feature_35 (0.005189)\n",
      "16. feature 546 feature_546 (0.005102)\n",
      "17. feature 294 feature_294 (0.004270)\n",
      "18. feature 246 feature_246 (0.004266)\n",
      "19. feature 49 feature_49 (0.004215)\n",
      "20. feature 413 feature_413 (0.004192)\n",
      "21. feature 326 feature_326 (0.004107)\n",
      "22. feature 525 feature_525 (0.004097)\n",
      "23. feature 666 feature_666 (0.004092)\n",
      "24. feature 660 feature_660 (0.004061)\n",
      "25. feature 129 feature_129 (0.003987)\n",
      "26. feature 74 feature_74 (0.003952)\n",
      "27. feature 538 feature_538 (0.003905)\n",
      "28. feature 348 feature_348 (0.003848)\n",
      "29. feature 304 feature_304 (0.003842)\n",
      "30. feature 279 feature_279 (0.003743)\n",
      "31. feature 96 feature_96 (0.003711)\n",
      "32. feature 126 feature_126 (0.003705)\n",
      "33. feature 300 feature_300 (0.003678)\n",
      "34. feature 40 feature_40 (0.003668)\n",
      "35. feature 11 feature_11 (0.003667)\n",
      "36. feature 489 feature_489 (0.003629)\n",
      "37. feature 316 feature_316 (0.003603)\n",
      "38. feature 231 feature_231 (0.003586)\n",
      "39. feature 668 feature_668 (0.003568)\n",
      "40. feature 540 feature_540 (0.003553)\n",
      "41. feature 505 feature_505 (0.003489)\n",
      "42. feature 41 feature_41 (0.003456)\n",
      "43. feature 353 feature_353 (0.003442)\n",
      "44. feature 653 feature_653 (0.003428)\n",
      "45. feature 155 feature_155 (0.003417)\n",
      "46. feature 305 feature_305 (0.003408)\n",
      "47. feature 385 feature_385 (0.003349)\n",
      "48. feature 218 feature_218 (0.003341)\n",
      "49. feature 29 feature_29 (0.003335)\n",
      "50. feature 346 feature_346 (0.003312)\n",
      "51. feature 68 feature_68 (0.003300)\n",
      "52. feature 138 feature_138 (0.003296)\n",
      "53. feature 89 feature_89 (0.003274)\n",
      "54. feature 503 feature_503 (0.003251)\n",
      "55. feature 547 feature_547 (0.003250)\n",
      "56. feature 51 feature_51 (0.003230)\n",
      "57. feature 243 feature_243 (0.003193)\n",
      "58. feature 599 feature_599 (0.003161)\n",
      "59. feature 476 feature_476 (0.003112)\n",
      "60. feature 7 feature_7 (0.003105)\n",
      "61. feature 596 feature_596 (0.003072)\n",
      "62. feature 463 feature_463 (0.003039)\n",
      "63. feature 171 feature_171 (0.002996)\n",
      "64. feature 15 feature_15 (0.002992)\n",
      "65. feature 508 feature_508 (0.002991)\n",
      "66. feature 19 feature_19 (0.002988)\n",
      "67. feature 211 feature_211 (0.002967)\n",
      "68. feature 642 feature_642 (0.002933)\n",
      "69. feature 206 feature_206 (0.002914)\n",
      "70. feature 88 feature_88 (0.002910)\n",
      "71. feature 442 feature_442 (0.002899)\n",
      "72. feature 268 feature_268 (0.002827)\n",
      "73. feature 626 feature_626 (0.002823)\n",
      "74. feature 342 feature_342 (0.002744)\n",
      "75. feature 226 feature_226 (0.002721)\n",
      "76. feature 486 feature_486 (0.002690)\n",
      "77. feature 148 feature_148 (0.002687)\n",
      "78. feature 212 feature_212 (0.002664)\n",
      "79. feature 329 feature_329 (0.002654)\n",
      "80. feature 258 feature_258 (0.002647)\n",
      "81. feature 504 feature_504 (0.002643)\n",
      "82. feature 475 feature_475 (0.002631)\n",
      "83. feature 629 feature_629 (0.002630)\n",
      "84. feature 371 feature_371 (0.002618)\n",
      "85. feature 27 feature_27 (0.002604)\n",
      "86. feature 625 feature_625 (0.002603)\n",
      "87. feature 351 feature_351 (0.002567)\n",
      "88. feature 252 feature_252 (0.002554)\n",
      "89. feature 150 feature_150 (0.002552)\n",
      "90. feature 663 feature_663 (0.002517)\n",
      "91. feature 533 feature_533 (0.002508)\n",
      "92. feature 290 feature_290 (0.002485)\n",
      "93. feature 20 feature_20 (0.002468)\n",
      "94. feature 569 feature_569 (0.002453)\n",
      "95. feature 490 feature_490 (0.002446)\n",
      "96. feature 87 feature_87 (0.002417)\n",
      "97. feature 65 feature_65 (0.002403)\n",
      "98. feature 103 feature_103 (0.002388)\n",
      "99. feature 293 feature_293 (0.002352)\n",
      "100. feature 285 feature_285 (0.002347)\n",
      "(64, 200)\n",
      "(64, 672)\n",
      "64\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -0.038135  -0.021344   0.050218  -0.045803  -0.033146   0.029607   \n",
      "1  -0.017397   0.003196   0.025299  -0.049143   0.020379   0.019599   \n",
      "2   0.008102  -0.026595  -0.038854   0.023067   0.000956  -0.032514   \n",
      "3   0.091391  -0.034410  -0.046202   0.010334  -0.039960   0.047707   \n",
      "4  -0.016237   0.023195   0.043193   0.013891   0.019974  -0.040611   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_662  feature_663  \\\n",
      "0   0.031293  -0.003199  -0.046154   0.029457  ...     0.021250     0.023258   \n",
      "1  -0.024797  -0.018282  -0.032134  -0.021385  ...    -0.085980     0.035986   \n",
      "2   0.024831   0.014583   0.015340  -0.048966  ...     0.034363     0.025496   \n",
      "3   0.017128   0.020622  -0.065581  -0.036677  ...    -0.007478     0.017845   \n",
      "4  -0.022273   0.096411  -0.062761  -0.067945  ...     0.028631     0.015119   \n",
      "\n",
      "   feature_664  feature_665  feature_666  feature_667  feature_668  \\\n",
      "0     0.024367     0.051106     0.024030     0.047877     0.068056   \n",
      "1     0.032514     0.034748    -0.036323     0.035792     0.082239   \n",
      "2     0.019904     0.025447     0.013342     0.008534     0.055924   \n",
      "3     0.016041     0.022515     0.015971    -0.019444    -0.009630   \n",
      "4     0.034319     0.034803     0.031954    -0.052447     0.023465   \n",
      "\n",
      "   feature_669  feature_670  feature_671  \n",
      "0    -0.088991     0.077928     0.023265  \n",
      "1     0.034407     0.104235     0.033394  \n",
      "2    -0.023555     0.022802     0.025472  \n",
      "3     0.016138     0.025911     0.012235  \n",
      "4     0.014136     0.021291     0.015567  \n",
      "\n",
      "[5 rows x 672 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature importance ranking\n",
      "1. feature 72 feature_72 (0.014767)\n",
      "2. feature 556 feature_556 (0.011712)\n",
      "3. feature 281 feature_281 (0.010941)\n",
      "4. feature 470 feature_470 (0.007511)\n",
      "5. feature 47 feature_47 (0.007307)\n",
      "6. feature 124 feature_124 (0.007186)\n",
      "7. feature 671 feature_671 (0.006258)\n",
      "8. feature 538 feature_538 (0.006111)\n",
      "9. feature 218 feature_218 (0.005753)\n",
      "10. feature 51 feature_51 (0.005704)\n",
      "11. feature 353 feature_353 (0.005563)\n",
      "12. feature 129 feature_129 (0.005333)\n",
      "13. feature 651 feature_651 (0.005213)\n",
      "14. feature 7 feature_7 (0.005068)\n",
      "15. feature 322 feature_322 (0.004977)\n",
      "16. feature 44 feature_44 (0.004933)\n",
      "17. feature 596 feature_596 (0.004930)\n",
      "18. feature 294 feature_294 (0.004825)\n",
      "19. feature 41 feature_41 (0.004745)\n",
      "20. feature 155 feature_155 (0.004729)\n",
      "21. feature 158 feature_158 (0.004538)\n",
      "22. feature 285 feature_285 (0.004430)\n",
      "23. feature 49 feature_49 (0.004388)\n",
      "24. feature 126 feature_126 (0.004171)\n",
      "25. feature 35 feature_35 (0.004092)\n",
      "26. feature 505 feature_505 (0.004068)\n",
      "27. feature 511 feature_511 (0.004000)\n",
      "28. feature 24 feature_24 (0.003995)\n",
      "29. feature 40 feature_40 (0.003987)\n",
      "30. feature 348 feature_348 (0.003960)\n",
      "31. feature 293 feature_293 (0.003870)\n",
      "32. feature 653 feature_653 (0.003847)\n",
      "33. feature 37 feature_37 (0.003847)\n",
      "34. feature 476 feature_476 (0.003749)\n",
      "35. feature 222 feature_222 (0.003644)\n",
      "36. feature 346 feature_346 (0.003635)\n",
      "37. feature 666 feature_666 (0.003577)\n",
      "38. feature 525 feature_525 (0.003547)\n",
      "39. feature 305 feature_305 (0.003487)\n",
      "40. feature 554 feature_554 (0.003479)\n",
      "41. feature 546 feature_546 (0.003465)\n",
      "42. feature 595 feature_595 (0.003462)\n",
      "43. feature 614 feature_614 (0.003414)\n",
      "44. feature 468 feature_468 (0.003413)\n",
      "45. feature 517 feature_517 (0.003412)\n",
      "46. feature 316 feature_316 (0.003268)\n",
      "47. feature 29 feature_29 (0.003267)\n",
      "48. feature 392 feature_392 (0.003197)\n",
      "49. feature 326 feature_326 (0.003159)\n",
      "50. feature 625 feature_625 (0.003127)\n",
      "51. feature 76 feature_76 (0.003120)\n",
      "52. feature 246 feature_246 (0.003110)\n",
      "53. feature 12 feature_12 (0.003106)\n",
      "54. feature 219 feature_219 (0.003089)\n",
      "55. feature 350 feature_350 (0.003061)\n",
      "56. feature 357 feature_357 (0.003049)\n",
      "57. feature 473 feature_473 (0.003042)\n",
      "58. feature 34 feature_34 (0.003033)\n",
      "59. feature 252 feature_252 (0.003001)\n",
      "60. feature 171 feature_171 (0.002983)\n",
      "61. feature 11 feature_11 (0.002976)\n",
      "62. feature 660 feature_660 (0.002973)\n",
      "63. feature 15 feature_15 (0.002957)\n",
      "64. feature 88 feature_88 (0.002942)\n",
      "65. feature 319 feature_319 (0.002863)\n",
      "66. feature 313 feature_313 (0.002826)\n",
      "67. feature 489 feature_489 (0.002808)\n",
      "68. feature 649 feature_649 (0.002798)\n",
      "69. feature 413 feature_413 (0.002795)\n",
      "70. feature 605 feature_605 (0.002794)\n",
      "71. feature 19 feature_19 (0.002778)\n",
      "72. feature 336 feature_336 (0.002757)\n",
      "73. feature 475 feature_475 (0.002716)\n",
      "74. feature 609 feature_609 (0.002704)\n",
      "75. feature 351 feature_351 (0.002700)\n",
      "76. feature 510 feature_510 (0.002681)\n",
      "77. feature 663 feature_663 (0.002666)\n",
      "78. feature 633 feature_633 (0.002665)\n",
      "79. feature 300 feature_300 (0.002639)\n",
      "80. feature 615 feature_615 (0.002601)\n",
      "81. feature 67 feature_67 (0.002577)\n",
      "82. feature 181 feature_181 (0.002570)\n",
      "83. feature 143 feature_143 (0.002549)\n",
      "84. feature 212 feature_212 (0.002540)\n",
      "85. feature 342 feature_342 (0.002528)\n",
      "86. feature 329 feature_329 (0.002514)\n",
      "87. feature 479 feature_479 (0.002513)\n",
      "88. feature 150 feature_150 (0.002510)\n",
      "89. feature 94 feature_94 (0.002498)\n",
      "90. feature 74 feature_74 (0.002493)\n",
      "91. feature 635 feature_635 (0.002451)\n",
      "92. feature 466 feature_466 (0.002425)\n",
      "93. feature 372 feature_372 (0.002412)\n",
      "94. feature 599 feature_599 (0.002409)\n",
      "95. feature 385 feature_385 (0.002405)\n",
      "96. feature 448 feature_448 (0.002404)\n",
      "97. feature 134 feature_134 (0.002398)\n",
      "98. feature 524 feature_524 (0.002386)\n",
      "99. feature 292 feature_292 (0.002376)\n",
      "100. feature 20 feature_20 (0.002374)\n",
      "(64, 250)\n",
      "(64, 672)\n",
      "64\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -0.038135  -0.021344   0.050218  -0.045803  -0.033146   0.029607   \n",
      "1  -0.017397   0.003196   0.025299  -0.049143   0.020379   0.019599   \n",
      "2   0.008102  -0.026595  -0.038854   0.023067   0.000956  -0.032514   \n",
      "3   0.091391  -0.034410  -0.046202   0.010334  -0.039960   0.047707   \n",
      "4  -0.016237   0.023195   0.043193   0.013891   0.019974  -0.040611   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_662  feature_663  \\\n",
      "0   0.031293  -0.003199  -0.046154   0.029457  ...     0.021250     0.023258   \n",
      "1  -0.024797  -0.018282  -0.032134  -0.021385  ...    -0.085980     0.035986   \n",
      "2   0.024831   0.014583   0.015340  -0.048966  ...     0.034363     0.025496   \n",
      "3   0.017128   0.020622  -0.065581  -0.036677  ...    -0.007478     0.017845   \n",
      "4  -0.022273   0.096411  -0.062761  -0.067945  ...     0.028631     0.015119   \n",
      "\n",
      "   feature_664  feature_665  feature_666  feature_667  feature_668  \\\n",
      "0     0.024367     0.051106     0.024030     0.047877     0.068056   \n",
      "1     0.032514     0.034748    -0.036323     0.035792     0.082239   \n",
      "2     0.019904     0.025447     0.013342     0.008534     0.055924   \n",
      "3     0.016041     0.022515     0.015971    -0.019444    -0.009630   \n",
      "4     0.034319     0.034803     0.031954    -0.052447     0.023465   \n",
      "\n",
      "   feature_669  feature_670  feature_671  \n",
      "0    -0.088991     0.077928     0.023265  \n",
      "1     0.034407     0.104235     0.033394  \n",
      "2    -0.023555     0.022802     0.025472  \n",
      "3     0.016138     0.025911     0.012235  \n",
      "4     0.014136     0.021291     0.015567  \n",
      "\n",
      "[5 rows x 672 columns]\n",
      "\n",
      "feature importance ranking\n",
      "1. feature 281 feature_281 (0.015664)\n",
      "2. feature 72 feature_72 (0.012474)\n",
      "3. feature 124 feature_124 (0.009786)\n",
      "4. feature 470 feature_470 (0.009590)\n",
      "5. feature 556 feature_556 (0.007807)\n",
      "6. feature 47 feature_47 (0.007579)\n",
      "7. feature 538 feature_538 (0.005904)\n",
      "8. feature 44 feature_44 (0.005824)\n",
      "9. feature 51 feature_51 (0.005583)\n",
      "10. feature 511 feature_511 (0.005439)\n",
      "11. feature 158 feature_158 (0.005377)\n",
      "12. feature 293 feature_293 (0.005375)\n",
      "13. feature 625 feature_625 (0.005316)\n",
      "14. feature 346 feature_346 (0.005204)\n",
      "15. feature 651 feature_651 (0.005115)\n",
      "16. feature 49 feature_49 (0.005090)\n",
      "17. feature 671 feature_671 (0.005061)\n",
      "18. feature 599 feature_599 (0.005057)\n",
      "19. feature 126 feature_126 (0.004877)\n",
      "20. feature 16 feature_16 (0.004772)\n",
      "21. feature 294 feature_294 (0.004767)\n",
      "22. feature 653 feature_653 (0.004728)\n",
      "23. feature 546 feature_546 (0.004558)\n",
      "24. feature 41 feature_41 (0.004513)\n",
      "25. feature 35 feature_35 (0.004389)\n",
      "26. feature 222 feature_222 (0.004374)\n",
      "27. feature 663 feature_663 (0.004341)\n",
      "28. feature 322 feature_322 (0.004298)\n",
      "29. feature 666 feature_666 (0.004145)\n",
      "30. feature 392 feature_392 (0.004014)\n",
      "31. feature 7 feature_7 (0.003996)\n",
      "32. feature 40 feature_40 (0.003973)\n",
      "33. feature 525 feature_525 (0.003854)\n",
      "34. feature 88 feature_88 (0.003706)\n",
      "35. feature 353 feature_353 (0.003614)\n",
      "36. feature 442 feature_442 (0.003546)\n",
      "37. feature 350 feature_350 (0.003526)\n",
      "38. feature 316 feature_316 (0.003495)\n",
      "39. feature 258 feature_258 (0.003488)\n",
      "40. feature 67 feature_67 (0.003473)\n",
      "41. feature 540 feature_540 (0.003424)\n",
      "42. feature 660 feature_660 (0.003402)\n",
      "43. feature 595 feature_595 (0.003392)\n",
      "44. feature 181 feature_181 (0.003368)\n",
      "45. feature 211 feature_211 (0.003328)\n",
      "46. feature 231 feature_231 (0.003255)\n",
      "47. feature 348 feature_348 (0.003253)\n",
      "48. feature 475 feature_475 (0.003229)\n",
      "49. feature 143 feature_143 (0.003227)\n",
      "50. feature 642 feature_642 (0.003177)\n",
      "51. feature 668 feature_668 (0.003165)\n",
      "52. feature 483 feature_483 (0.003119)\n",
      "53. feature 74 feature_74 (0.003110)\n",
      "54. feature 218 feature_218 (0.003089)\n",
      "55. feature 486 feature_486 (0.003025)\n",
      "56. feature 171 feature_171 (0.003024)\n",
      "57. feature 554 feature_554 (0.003001)\n",
      "58. feature 503 feature_503 (0.002971)\n",
      "59. feature 326 feature_326 (0.002950)\n",
      "60. feature 339 feature_339 (0.002925)\n",
      "61. feature 547 feature_547 (0.002902)\n",
      "62. feature 19 feature_19 (0.002900)\n",
      "63. feature 596 feature_596 (0.002870)\n",
      "64. feature 155 feature_155 (0.002855)\n",
      "65. feature 34 feature_34 (0.002841)\n",
      "66. feature 476 feature_476 (0.002830)\n",
      "67. feature 533 feature_533 (0.002809)\n",
      "68. feature 614 feature_614 (0.002805)\n",
      "69. feature 458 feature_458 (0.002800)\n",
      "70. feature 219 feature_219 (0.002797)\n",
      "71. feature 331 feature_331 (0.002756)\n",
      "72. feature 29 feature_29 (0.002754)\n",
      "73. feature 248 feature_248 (0.002749)\n",
      "74. feature 94 feature_94 (0.002741)\n",
      "75. feature 466 feature_466 (0.002720)\n",
      "76. feature 605 feature_605 (0.002718)\n",
      "77. feature 220 feature_220 (0.002715)\n",
      "78. feature 226 feature_226 (0.002706)\n",
      "79. feature 623 feature_623 (0.002664)\n",
      "80. feature 202 feature_202 (0.002664)\n",
      "81. feature 107 feature_107 (0.002658)\n",
      "82. feature 444 feature_444 (0.002658)\n",
      "83. feature 313 feature_313 (0.002581)\n",
      "84. feature 89 feature_89 (0.002563)\n",
      "85. feature 76 feature_76 (0.002560)\n",
      "86. feature 37 feature_37 (0.002476)\n",
      "87. feature 656 feature_656 (0.002474)\n",
      "88. feature 367 feature_367 (0.002455)\n",
      "89. feature 420 feature_420 (0.002452)\n",
      "90. feature 574 feature_574 (0.002446)\n",
      "91. feature 510 feature_510 (0.002414)\n",
      "92. feature 489 feature_489 (0.002412)\n",
      "93. feature 626 feature_626 (0.002407)\n",
      "94. feature 285 feature_285 (0.002386)\n",
      "95. feature 129 feature_129 (0.002378)\n",
      "96. feature 271 feature_271 (0.002377)\n",
      "97. feature 446 feature_446 (0.002365)\n",
      "98. feature 80 feature_80 (0.002363)\n",
      "99. feature 505 feature_505 (0.002346)\n",
      "100. feature 669 feature_669 (0.002346)\n",
      "(64, 300)\n"
     ]
    }
   ],
   "source": [
    "#feature selection\n",
    "kernels = [16, 32]\n",
    "feat_numbers = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "for kernel in kernels:\n",
    "    for feat_number in feat_numbers:\n",
    "        save_dir = 'FINAL_RESULTS_60'\n",
    "        save_dir = save_dir + '/'+feature_type+'_timesteps'+str(timesteps)+'_HiddenRatio'+str(hidden_ratio)+'/fisherVectors'\n",
    "\n",
    "        X_train = np.load(os.path.join(save_dir, 'fisher_vector_train_%i.npy' %kernel))\n",
    "        \n",
    "\n",
    "        print (X_train.shape)\n",
    "        print (len(y_train))\n",
    "        \n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        model = RandomForestClassifier(n_estimators=800, criterion='entropy')\n",
    "        df = pd.DataFrame(X_train)\n",
    "        y = y_train\n",
    "        feature_names = ['feature_%d' % i for i in range(len(X_train[0]))]\n",
    "        df.columns = feature_names\n",
    "        print(df.head())\n",
    "\n",
    "        model.fit(df, y)\n",
    "        importances = model.feature_importances_\n",
    "        print(\"\\nfeature importance ranking\")\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        for f in range(100):\n",
    "            print(\"%d. feature %d %s (%f)\" % (f+1, indices[f], feature_names[indices[f]], importances[indices[f]]))\n",
    "        indices = indices[:feat_number]\n",
    "        #np.save(os.path.join(save_dir, 'feature_list_%d' % 32), indices)\n",
    "\n",
    "        X_train_df = pd.DataFrame(X_train)\n",
    "        X_train_df.columns = ['feature_%d' % i for i in range(len(X_train[0]))]\n",
    "        X_train_tree = X_train_df.iloc[:, indices]\n",
    "\n",
    "       \n",
    "\n",
    "        print(X_train_tree.shape)\n",
    "\n",
    "        save_dir = 'FINAL_RESULTS_60'\n",
    "        save_dir = save_dir + '/'+feature_type+'_timesteps'+str(timesteps)+'_HiddenRatio'+str(hidden_ratio)\n",
    "\n",
    "        np.save(os.path.join(save_dir, 'X_train_tree_%i_%i' %(kernel, feat_number)), X_train_tree)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running Random Forest with Cross Validation on Fisher Vectors\n",
      "Kernels 16 and features number 50\n",
      "0.7097165991902834\n",
      "Kernels 16 and features number 100\n",
      "0.6473429951690821\n",
      "Kernels 16 and features number 150\n",
      "0.647536231884058\n",
      "Kernels 16 and features number 200\n",
      "0.5550682261208577\n",
      "Kernels 16 and features number 250\n",
      "0.48256410256410254\n",
      "Kernels 16 and features number 300\n",
      "0.5693257359924027\n",
      "Kernels 32 and features number 50\n",
      "0.697979797979798\n",
      "Kernels 32 and features number 100\n",
      "0.7835968379446641\n",
      "Kernels 32 and features number 150\n",
      "0.572192513368984\n",
      "Kernels 32 and features number 200\n",
      "0.6018474279343843\n",
      "Kernels 32 and features number 250\n",
      "0.6945373467112598\n",
      "Kernels 32 and features number 300\n",
      "0.5715738621917111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"\\nrunning Random Forest with Cross Validation on Fisher Vectors\")\n",
    "\n",
    "kernels = [16, 32]\n",
    "feat_numbers = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "for kernel in kernels:\n",
    "    for feat_number in feat_numbers:\n",
    "        print ('Kernels %i and features number %i' %(kernel, feat_number))\n",
    "\n",
    "        save_dir = 'FINAL_RESULTS_60'\n",
    "        save_dir = save_dir + '/'+feature_type+'_timesteps'+str(timesteps)+'_HiddenRatio'+str(hidden_ratio)\n",
    "\n",
    "        X_train = np.load(os.path.join(save_dir, 'X_train_tree_%i_%i.npy' %(kernel, feat_number)))\n",
    "        clf = RandomForestClassifier() \n",
    "        print (np.mean(cross_val_score(clf, X_train, y_train, cv=3, scoring = \"f1\")))\n",
    "        \n",
    "        \n",
    "        filename = os.path.join(save_dir, '%s_%s_score.json' % (kernel, feat_number))\n",
    "        file = open(filename,\"w\") \n",
    "        file.write(\"\\nf1score 3 fold CV: %.4f\" % np.mean(cross_val_score(clf, X_train, y_train, cv=3, scoring = \"f1\")))\n",
    "        #file.write(\"\\naccuracy 3 fold CV: %.4f\" % np.mean(cross_val_score(clf, X_train, y_train, cv=3, scoring = \"accuracy\")))\n",
    "        #file.write(\"\\nprecision 3 fold CV: %.4f\" % np.mean(cross_val_score(clf, X_train, y_train, cv=3, scoring = \"precision\")))\n",
    "        #file.write(\"\\nrecall 3 fold CV: %.4f\" % np.mean(cross_val_score(clf, X_train, y_train, cv=3, scoring = \"recall\")))\n",
    "        file.close() \n",
    "        \n",
    "        \n",
    "        \n",
    "         \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
